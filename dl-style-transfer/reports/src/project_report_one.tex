\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Neural Style Transfer on Text\\Project Report One}

\author{Yuji Akimoto, Ryan Butler, Cameron Ibrahim, Luca Leeser}

\begin{document}

\maketitle

\section*{Abstract}

The current state of the project is rather fluid. Having studied current advances
in Style Trasfer and recreated Gatys' original model, we are currently studying
the feasibility of style transfer on text using a similar method. There are a number
of obstacles in this regard, including finding a feasible model for text reconstruction,
studying the properties of style in text, and the possibility of connecting these
two concepts in an extension of existing style transfer work. This group is currently
working on the development of 

\section{Introduction}

The current objective of our work has been to research the viability of a variety of text representation models, and the viability of text reconstruction using these methods. Work has been done in using CNNs and autoencoders for text representation. Currently,
the models are still in the process of being implemented, an analysis of their viability for text representation is pending.


\section{Methods and Results}

At this point in the process, the team is still constructing a number of different models
for text representation. Viable candidates include, CNNs, Varaitional Autoencoders, and KATE. Using these, we will study the possiblity of text representation over the next week.

\section{Individual Work}

\begin{description}

	\item [Yuji Akimoto]
		Implemented image style transfer with VGG-16 trained on CIFAR-10 data instead of ImageNet data, to see if ``content'' representations of images would still be general enough. Also implemented (untrained) ``Shallow-CNN'', a popular convolutional architecture for text classification as described by [Yoon Kim](https://arxiv.org/abs/1408.5882).

	\item [Ryan Butler]

	\item [Cameron Ibrahim]
		Attempting to implement the KATE autoencoder.
		Generated vector representations of the Yelp review vocabulary. 

	\item [Luca Leeser]
		Implemented a variational autoencoder.

\end{description}

\section{Further Studies}

We intend to study the layer by layer representation of text in our models and intend to implement text reconstruction. From there, we will begin to study style representation of text.

\section{References}

\begin{thebibliography}{9}
	\bibitem{Neural Style}
	Leon A. Gatys, Alexander S. Ecker, Matthias Bethge.
	\textit{A Neural Algorithm of Artistic Style}.
	CoRR, 2015.

	\bibitem{Style Transfer Summary}
	Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Mingli Song.
	\textit{Neural Style Transfer: A Review}.
	CoRR, 2017.

	\bibitem{Generative models}
	Justin Johnson, Alexandre Alahi, Fei-Fei Li.
	\textit{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}.
	Stanford University.

	\bibitem{Markovian GANs}
	Chuan Li, Michael Wand.
	\textit{Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks}.
	CoRR, 2016.

	\bibitem{Writing Style Conversion}
	Se Wong Jang, Jesik Min, Mark Kwon.
	\textit{Writing Style Conversion using Neural Machine Translation}.
	Stanford University.

	\bibitem{Text style transfer}
	Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola.
	\textit{Style Transfer from Non-Parallel Text by Cross-Alignment}.
	CoRR, 2017.

	\bibitem{Autoencoder}
	Barak Oshri, Nishith Khandwala.
	\textit{There and Back Again: Autoencoders for Textual Reconstruction}.
	Stanford University.

	\bibitem{KATE}
	Yu Chen, Mohammed J. Zaki.
	\textit{KATE: K-Competitive Autoencoder for Text}. Rensselaer Polytechnic Institute.
	
	\bibitem{Shallow-CNN}
	Yoon Kim.
	\textit{Convolutional Neural Networks for Sentence Classification}. CoRR, 2014.

\end{thebibliography}


\end{document}
