{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "An implementation of neural style transfer for images as described in the paper [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) by Gatys et. al in 2015. We use a Convolutional Neural Network to transfer the style of one image on to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf  \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OUTPUT_DIR`: directory in which to save style-transferred image  \n",
    "`STYLE_IMAGE`: image from which to transfer the style    \n",
    "`CONTENT_IMAGE`: image on which to transfer the style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'output/'\n",
    "STYLE_IMAGE = 'images/great-wave.jpg'\n",
    "CONTENT_IMAGE = 'images/cornell-campus.jpg'\n",
    "\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "COLOR_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content Image              |  Style Image\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/cornell-campus.jpg)  |  ![](images/great-wave.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOISE_RATIO`: we create the style-transferred image by starting from white noise, and iteratively updating pixels until the content matches the content image, and the style image. It has been shown that starting from white noise over the content image results in faster performance than starting from pure white noise - here, the ratio is the proportion of our starting images which is composed of noise (the rest will be from the content image.  \n",
    "  \n",
    "`ALPHA`, `BETA`: these hyperparameters control how to balance trying to match the content of the content image, and the style of the style image. Specifically, these are the weights we assign to the content and style terms of the loss function respectively. A higher $\\frac{\\alpha}{\\beta}$ results in an image that preserves more of the content, while a lower $\\frac{\\alpha}{\\beta}$ results in an image much similar to the style image.  \n",
    "  \n",
    "`VGG_MODEL`: we use a pre-trained VGG-19 network to determine both the content and the style of an image. VGG-19 is a convolutional neural network architecture used for image classification, so the features it extracts to use for classification are a good representation of the content, and the correlations between these features represent the style of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOISE_RATIO = 0.6\n",
    "ALPHA = 1\n",
    "BETA = 1000\n",
    "\n",
    "VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'\n",
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to load the pre-trained weights of the VGG-19 model. These (and many others) can be downloaded from [here](http://www.vlfeat.org/matconvnet/pretrained/).   \n",
    "Although VGG-19 uses max-pooling, Gatys et. al found that using average pooling produces better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vgg_model(path):\n",
    "    \"\"\"\n",
    "    Load a pre-trained VGG-19 network from weights given in `path`.\n",
    "    \"\"\"\n",
    "    \n",
    "    vgg = scipy.io.loadmat(path)\n",
    "    vgg_layers = vgg['layers']\n",
    "    \n",
    "    def _weights(layer):\n",
    "        W = vgg_layers[0][layer][0][0][0][0][0]\n",
    "        b = vgg_layers[0][layer][0][0][0][0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][-2]\n",
    "        return W, b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer):\n",
    "        W, b = _weights(layer)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.relu(tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b)\n",
    "\n",
    "    def _avgpool(prev_layer):\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Constructs the graph model.\n",
    "    graph = {}\n",
    "    graph['input']    = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')\n",
    "    \n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'],     0)\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'],   2)\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    \n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'],  5)\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'],   7)\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    \n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10)\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'],  12)\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'],  14)\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'],  16)\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    \n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19)\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'],  21)\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'],  23)\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'],  25)\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    \n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28)\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'],  30)\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'],  32)\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'],  34)\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any given layer in a convolutional neural network encodes the image by the activations of filters (feature detectors), and in later layers these encode \"content\" that can be interpreted by the fully connected layers to produce classification predictions. Gatys et. al found that comparing the encoding of two images in the 4th convolutional layer was a good way to measure the difference in content.  \n",
    "  \n",
    "Specifically, we take the squared-error between the two feature representations. Let $p$ and $x$ be the original and generated image respectively, and let $P^l$ and $F^l$ be their feature representations in layer $l$. Then:  \n",
    "$$E_{loss}(p,x,l) = \\frac{1}{2} \\sum_{i,j} (P^l_{ij} - F^l_{ij})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Content loss function as defined in the paper by Gatys et. al\n",
    "    \"\"\"\n",
    "    def _content_loss(l, x):\n",
    "        N = l.shape[3]              # number of filters (at layer l)\n",
    "        M = l.shape[1] * l.shape[2] # height times the width of the feature map (at layer l)\n",
    "        # return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - l, 2))\n",
    "        return 0.5 * tf.reduce_sum(tf.pow(x - l, 2))\n",
    "    \n",
    "    return _content_loss(sess.run(model['conv4_2']), model['conv4_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the style of an image by taking the Gram matrix of the feature representations. For any matrix $A$, the Gram matrix is defined as $A^TA$ - this matrix has many interpretations, but for our purpose this measures the correlations between activations of the feature map. Gatys et. al found that combining the style representation at each convolutional layer produced the best representation of the overall style of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('conv1_1', 0.5),\n",
    "    ('conv2_1', 1.0),\n",
    "    ('conv3_1', 1.5),\n",
    "    ('conv4_1', 3.0),\n",
    "    ('conv5_1', 4.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we take the squared-error between the style representations of two images, and scale according to the number of filters and the sibe of the activation map. Let $a$ and $x$ be the original and generated image respectively, and let $A^l$ and $G^l$ be their style representations in layer $l$. Then:  \n",
    "$$E_{style}(a,x) = \\sum_{l} \\frac{w_l}{4N_l^2M_l^2} \\sum_{i,j} (G^l_{ij} - A^l_{ij})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def style_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Style loss function as defined in the paper by Gatys et. al\n",
    "    \"\"\"\n",
    "    def _gram_matrix(F, N, M):\n",
    "        Ft = tf.reshape(F, (M, N))\n",
    "        return tf.matmul(tf.transpose(Ft), Ft)\n",
    "\n",
    "    def _style_loss(l, x):\n",
    "        N = l.shape[3]              # number of filters (at layer l)\n",
    "        M = l.shape[1] * l.shape[2] # height times the width of the feature map (at layer l)\n",
    "        \n",
    "        A = _gram_matrix(l, N, M)   # style representation of the original image (at layer l)\n",
    "        G = _gram_matrix(x, N, M)   # style representation of the generated image (at layer l)\n",
    "        \n",
    "        return (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2))\n",
    "\n",
    "    E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in STYLE_LAYERS]\n",
    "    W = [w for _, w in STYLE_LAYERS]\n",
    "    loss = sum([W[l] * E[l] for l in range(len(STYLE_LAYERS))])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Gatys et. al proposed starting from white noise before iteratively decreasing both the content loss and style loss, it has since been shown that starting from some combination of white noise and the content image gives faster results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_noise_image(content_image, noise_ratio=NOISE_RATIO):\n",
    "    \"\"\"\n",
    "    Returns a noise image intermixed with the content image at a certain ratio.\n",
    "    \"\"\"\n",
    "    noise_image = np.random.uniform(-20, 20, (1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)).astype('float32')\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    return input_image\n",
    "\n",
    "def load_image(path):\n",
    "    image = scipy.misc.imread(path)\n",
    "    image = np.reshape(image, ((1,) + image.shape)) # reshape to Tensor\n",
    "    image = image - MEAN_VALUES                     # VGG expects image with mean subtracted\n",
    "    return image\n",
    "\n",
    "def save_image(path, image):\n",
    "    image = image + MEAN_VALUES                     # add mean to reproduce original colours\n",
    "    image = image[0]                                # reshape from Tensor to Matrix\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_image = load_image(CONTENT_IMAGE)\n",
    "style_image = load_image(STYLE_IMAGE)\n",
    "\n",
    "model = load_vgg_model(VGG_MODEL)\n",
    "input_image = generate_noise_image(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "sess.run(model['input'].assign(content_image))\n",
    "content_loss = content_loss_func(sess, model)\n",
    "\n",
    "sess.run(model['input'].assign(style_image))\n",
    "style_loss = style_loss_func(sess, model)\n",
    "\n",
    "total_loss = ALPHA*content_loss + BETA*style_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(2.0)\n",
    "train_step = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "sum :  -1.70221e+06\n",
      "cost:  2.06285e+12\n",
      "Iteration 100\n",
      "sum :  488755.0\n",
      "cost:  1.53288e+11\n",
      "Iteration 200\n",
      "sum :  1.91676e+06\n",
      "cost:  8.66659e+10\n",
      "Iteration 300\n",
      "sum :  3.06876e+06\n",
      "cost:  6.12483e+10\n",
      "Iteration 400\n",
      "sum :  3.99233e+06\n",
      "cost:  4.95293e+10\n",
      "Iteration 500\n",
      "sum :  4.75839e+06\n",
      "cost:  4.28567e+10\n",
      "Iteration 600\n",
      "sum :  5.42055e+06\n",
      "cost:  3.86298e+10\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(model['input'].assign(input_image))\n",
    "\n",
    "for it in range(ITERATIONS):\n",
    "    sess.run(train_step)\n",
    "    if it%100 == 0:\n",
    "        # Print every 100 iterations.\n",
    "        mixed_image = sess.run(model['input'])\n",
    "        print('Iteration %d' % (it))\n",
    "        print('sum : ', sess.run(tf.reduce_sum(mixed_image)))\n",
    "        print('cost: ', sess.run(total_loss))\n",
    "\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "        filename = 'output/%d.png' % (it)\n",
    "        save_image(filename, mixed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_image('output/art.jpg', mixed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content Image              |  Style Image              | Mixed Image\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "![](images/cornell-campus.jpg)  |  ![](images/great-wave.jpg)|  ![](output/500.png)\n",
    "![](images/gates-hall.jpg)  |  ![](images/starry_night.jpg)|  ![](output/gates-starry.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
