\documentclass{vldb}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage[capposition=top]{floatrow}
\usepackage{float}
\usepackage[caption = false]{subfig}
%\usepackage{hyperref}

\begin{document}

% ****************** TITLE ****************************************

\title{On the Use of K-Competitive Networks \\ for Writing Style Transfer}

% ****************** AUTHORS **************************************

\numberofauthors{3} 

\author{
\alignauthor
Luca Leeser\\
       \affaddr{Cornell University}\\
       \email{ll698@cornell.edu}
\alignauthor
Yuji Akimoto\\
       \affaddr{Cornell University}\\
       \email{ya242@cornell.edu}
\and
\alignauthor 
Ryan Butler\\
       \affaddr{Cornell University}\\
       \email{rjb392@cornell.edu}
\alignauthor 
Cameron Ibrahim\\
       \affaddr{Cornell University}\\
       \email{cai29@cornell.edu}
}

\date{1 December 2017}

\maketitle

\begin{abstract}
Writing style is a key component of the quality of any online review - a detailed and informative review is only useful insofar as it is able to retain a reader's attention. In this paper, we explore the use of $k$-competitive layers in convolutional neural networks to classify the author of a given piece of text. We then apply the techniques used in neural style transfer for images to transfer the writing style of one author onto the content written by another. Given sufficient examples of well-written reviews, our techniques can be used to improve the quality of reviews on online platforms such as Yelp or Amazon.
\end{abstract}

% INTRODUCTION
\section{Introduction}
Neural style transfer \cite{GatysEB15a} has proven very successful at transferring the artistic style of an image onto the content of another. Due to the layered structure of convolutional neural networks, it was observed that later layers in the network produce feature representations of the content of an image, while the correlations between these features give a good representation of artistic style. More specifically, given a content image and style image, the algorithm treats the input to the network as a variable and simultaneously minimizes the $\ell_2$ norm between the input and content image's feature representations, and the input and style image's style representations.

\begin{figure}[h]
\subfloat{\includegraphics[width = .3\linewidth]{gates-hall.jpg}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .3\linewidth]{starry_night.jpg}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .3\linewidth]{gates-starry.png}}
\caption{\textmd{Cornell's Gates Hall stylized in the form of Vincent Van Gogh's \textit{Starry Night}.}}
\end{figure} 

Modifications that optimize computational time \cite{JohnsonAL16}, preserve the texture of the style image \cite{GatysEB15}, and preserve facial structures in the content image \cite{KaurZD17}, among others, have further advanced the art of neural style transfer for images. However, applying similar techniques to text has proven difficult for a number of reasons. Firstly, textual data lives in an extremely sparse space, and therefore it is difficult to iteratively produce better stylized text while avoiding phrases that make no sense. Secondly, style and content are much more closely linked in text than they are in images - the same writer may write two passages of text in a completely different tone, yet maintain the same underlying style in a manner that is very subtle.

In this paper, we explore the use of $k$-competitive convolutional layers, a generalization of $k$-competitive layers designed for autoencoders in \cite{KATE}, which in turn are based on $k$-sparse layers as defined in \cite{MakhzaniF13}. These layers are integrated into a word-level convolutional neural network \cite{Kim14f}, and we perform a qualitative analysis of the quality of content and style representations generated by these networks. A key benefit of our method is that it does not require a training set of translations between writing styles, which are very difficult to obtain.

% RELATED WORK
\section{Related Work}
Early methods at quantitatively determining the writing style of a piece of text focused on statistical analyses of the rates of passive and active sentences, the ratio of pronouns to nouns to adjectives, and the likelihood of adjacent words given their distribution in the English language. However, these methods rely on unreliable tokenizations and are difficult to adapt to style transfer. Xu et. al. \cite{Xu12} provided some of the first algorithmic style transfer methods between Shakespearean and modern English, but relied on a phrase table and did not take advantage of modern machine learning methods. 

Recent methods based on neural networks fall into two broad categories: those that treat the problem as a mono-linguistic machine translation problem, and those that attempt to generate style and content representations of text.  \cite{Carlson17} and \cite{Jang16} fall under this first category, and generate stylized text that perform well according to BLEU score \cite{Papineni02}. However, these translation-based methods require a large dataset of tupled phrases that are identical in content and differ only in style. Use of passages in the Bible in \cite{Carlson17} and SparkNotes' Shakespearean translations in \cite{Jang16} present interesting but impractical test cases, while generating rap lyric translations with Google Translate via French \cite{Jang16} leaves much to be desired in terms of accuracy. 

\cite{Edirisooriya} and \cite{Kabbara16} both consider a similar method to ours in terms of generating style and content representations of text, and \cite{Kabbara16} defines three evaluation criteria: soundness (preservation of content), coherence (grammatical correctness \& common sense), and effectiveness (matches intended style). However, these lack sufficient experiments with which to evaluate the quality of their results. \cite{Fu17} provides a quantitative assessment of their content and style representations, in addition to a more complex alternative architecture. All of the aforementioned works use a recurrent neural network encoder-decoder structure (otherwise referred to as a ``seq2seq'' model), with either a long short-term memory (LSTM) or gated recurrent unit (GRU) used as the underlying structure. These methods seek to encode the content of the input text into a low dimensional space, and then use the decoder to generate text in a specified style. Our key contribution is in the use of convolutional neural networks, which have proven very successful at tasks such as sentiment analysis \cite{Kim14f}.

% PRELIMINARY ANALYSIS
\section{Preliminary Analysis}
The majority of implementations of style transfer for images use the VGG-19 \cite{VGG19} architecture to generate content and style representations of an image. This network was trained to classify images into one of the one thousand classes of the ImageNet challenge dataset \cite{ILSVRC15} and therefore is able to generate informative feature representations of a very large variety of objects. Given the resources required to train a similarly robust classifier for text, we first explore the tradeoffs of using an underlying network that is trained to discriminate between a much smaller number of classes by training a VGG network with the CIFAR-10 dataset \cite{Krizhevsky09} (here, we use pre-trained weights provided by \cite{Liu15}). Although this modified network is far less generalizable in terms of classification, we found that the content representations provided by later layers of the network were still informative enough to reconstruct the original image, even for images containing objects not included in the 10 classes of CIFAR-10. By reconstruction, we refer to treating the input to the network as a variable, and minimizing $\ell_2$ loss between the input image and original image's feature representations in a certain layer.

\begin{figure}[h]
\subfloat{\includegraphics[width = .15\linewidth]{cornell-campus-small.jpg}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{anderson-small.jpg}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{boats.jpg}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{whale.jpg}}
\hspace{0.1cm} 
\subfloat{\includegraphics[width = .15\linewidth]{pizza.jpg}}
\hspace{0.1cm} \\
\subfloat{\includegraphics[width = .15\linewidth]{2600.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{anderson5800.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{boats2100.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{boats3200.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{pizza5000.png}}
\hspace{0.1cm} \\
\subfloat{\includegraphics[width = .15\linewidth]{corne9900.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{ander9900.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{boats9900.png}}
\hspace	{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{whale9900.png}}
\hspace{0.1cm}
\subfloat{\includegraphics[width = .15\linewidth]{pizza9900.png}}
\hspace{0.1cm}
\caption{\textmd{Several 32x32 images (top) reconstructed in the conv2\_2 layer (middle) and conv4\_2 layer (bottom) of VGG-19 trained on CIFAR-10 data; maximum 10000 iterations.}}
\end{figure} 

In the reconstruction phase, we use average-pooling in place of max-pooling, as suggested in \cite{GatysEB15a}. Although none of the images in Figure 2 resemble any of the ten classes present in CIFAR-10 data, the network still produces highly informative feature representations through convolution. This is a key observation in making the transition to text data, as writing style is subjective and therefore it is difficult to collect data from a large number of distinct classes that span the entire space of writing styles. 

However, we also observe that this reduction in generalizability of the underlying network architecture is not without tradeoffs. When using the conv4\_2 layer at which to match the feature representations of two images (the layer used in Figure 1), the reconstructed images are mostly unrecognizable to the human eye, and takes a much larger number of iterations to produce something even remotely close to the original. While we do not dismiss the possibility that this loss in reconstruction accuracy could be mitigated by hyperparameter optimization, we conclude that high generalizability as a classifier is an important quality of the underlying architecture although not paramount.

% MODEL
\section{Model}
\subsection{Convolutional Neural Networks for Text Classification}
Although recurrent neural networks have proven adept at language recognition tasks, evaluating the overall content and style of written text is a task that is less dependent on sequential information, and a hidden state is likely too small to encode enough information for our purposes. We believe that a a convolutional neural network allows us to take advantage of the success of style transfer for images, in contrast to aforementioned approaches that treat style transfer as a machine translation problem.

As the baseline for our comparisons we use a word-level convolutional neural network as designed by Yoon Kim in \cite{Kim14f}. A 1-d convolutional layer varies from those commonly found in architectures for image classification in that the receptive field of a filter is one-dimensional, and subsequent pooling layers operate along the time axis. The network takes as input a sentence, with each word encoded as a vector. As a ``shallow'' architecture, the network performs layer of convolution with filter lengths of 3, 4, 5 with 100 feature maps each, before a max-over-time pooling operation and a softmax layer. 

\begin{figure}[h]
\includegraphics[width=\linewidth]{cnn.png}
\caption{\textmd{Structure of the baseline convolutional neural network model.}}
\end{figure}

\subsection{K-Competitive Convolutional Layers}
It has been observed that encouraging sparsity when learning representations acts as a capable reguarlizer that improves performance on classification tasks. In \cite{MakhzaniF13}, Makhzani and Frey designed the $k$-sparse autoencoder and demonstrated its effectiveness as a preprocessing step on image data. In the feedforward phase, sparsity is used as the sole non-linearity: after computing the activations $z = W^Tx + b$, only the largest $k$ entries of $z$ are kept and the rest are set to 0. 

Since textual data often has high-dimensionality, sparsity, and power-law distributions \cite{KATE}, autoencoders for text have often learned trivial features such as proper nouns specific to a certain passage. Chen and Zaki made a modification to $k$-sparse layers by introducing competition in the place of truncation: after computing the activations $z = tanh(W^Tx + b)$, only the $k$ entries with the largest absolute value are kept non-zero. These $k$ most ``competitive'' neurons are then augmented (with an amplification constant) with the \textbf{energy} of the ``losers'' that were made inactive, where the energy of a subset of neurons $H$ is defined as $\sum_{h_i \in H} |z_i|$. 

\begin{figure}[h]
\centering
\includegraphics[width=.8\linewidth]{k_complayer.png}
\caption{\textmd{A fully connected $k$-competitive layer.}}
\end{figure}

Here, we propose a generalization of $k$-competitive layers to convolutional layers. Let $z_{ijk}$ be the activation volume produced by a convolutional layer, where $z_{x,y,:}$ denotes the feature vector at spatial location $(x,y)$. Then, we define the \textbf{energy map} of an activation volume, $E$, as the matrix with entries:
$$e_{ij} = \sum_{k=1}^K |z_{ijk}|$$
We then perform a $k$-competitive non-linearity, where we select the $k$ most competitive entries, and masking the activations of the ``loser'' neurons while augmenting the activations of the ``winner'' neurons. If we denote the set of indices of the $k$ ``winner'' neurons by $C_k$, then:
$$kcomp(z)_{i,j,:} = \textbf{0} \hspace{0.5cm} \forall (i,j) \notin C_k$$
$$kcomp(z)_{i,j,:} = z_{i,j,:} + \alpha\sum_{(i,j) \notin C_k} z_{i,j,:} \hspace{0.5cm} \forall (i,j) \in C_k$$
where $\alpha$, the amplification constant, is a hyperparameter.

% EXPERIMENTS
\section{Experiments}
\subsection{Datasets}
We use the most prolific reviewers on Yelp\footnote{{\texttt{https://www.yelp.com/dataset/challenge}}} to generate a large amounts of text for various styles of writing. Although the reviews for a frequent user of Yelp may span a number of years, we make the assumption that a user's writing style remains constant over that period of time. This may not strictly be the case as language, particularly of the colloquial variety that is often found online, evolves over time and certain words serve as clear indicators of certain time periods. However, dealing with a time-varying vocabulary set is an extension that we have not yet considered. After identifying the most frequent reviewers, we tokenize all of their reviews using NLTK \cite{Loper02nltk:the} to generate a corpus. 

\subsection{Comparison of Architectures}

\subsection{Hyperparameters and Training}

% RESULTS
\section{Results}

% CONCLUSION
\section{Conclusion}

% FURTHER DISCUSSION
\section{Future Extensions}

% ACKNOWLEDGMENTS 
\section{Acknowledgments}
The authors would like to thank Professor Thorsten Joachims at the department of Computer Science at Cornell University for his guidance, and their fellow members of the Cornell Data Science NLP Research Team for their support.

\bibliographystyle{abbrv}
\bibliography{sources.bib}  


% APPENDIX
%pagebreak

\begin{appendix}
%\href{https://github.com/CornellDataScience/Yelp-FA17/tree/master/dl_style_transfer}{Link} to Github repository for all code associated with this paper. 

\end{appendix}
\end{document}