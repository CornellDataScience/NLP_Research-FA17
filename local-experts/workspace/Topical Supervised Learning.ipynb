{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Setups (Sklearn Library)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Runs the model specified by the clf (classifier), and features.\n",
    "Features is assumed to have an 'is_expert' column which gives the classifications'''\n",
    "def run_model(clf, features):\n",
    "    train, test = train_test_split(X)\n",
    "    train_classifier = train['is_expert'].values\n",
    "    test_classifier = test['is_expert'].values\n",
    "    train = train.drop('is_expert',axis=1)\n",
    "    test = test.drop('is_expert', axis=1)\n",
    "    clf.fit(train, train_classifier)\n",
    "    rf_pred = clf.predict(test)\n",
    "    model_perf= {'Model_Score' : clf.score(test, test_classifier),\n",
    "                'Predictions' : rf_pred,\n",
    "                'Actual' : test_classifier,\n",
    "                'Prediction_Probabilities' : clf.predict_proba(test),\n",
    "                'Total_Tested' : len(rf_pred),\n",
    "                'Num_Experts_Predicted' : sum(rf_pred),\n",
    "                'Num_Experts_Actual' : sum(test_classifier),\n",
    "                'Num_Experts_Training' : sum(train_classifier)} \n",
    "    return model_perf\n",
    "\n",
    "'''Runs the model n times, and prints out a dictionary with the statistics'''\n",
    "def bootstrap_model(clf, features, n):\n",
    "    models = []\n",
    "    for i in range(1,n):\n",
    "        models.append(run_model(clf, features))\n",
    "    return models\n",
    "\n",
    "'''Gets statistics from the bootstrap list of dictionaries'''\n",
    "def boot_statistics(models):\n",
    "    stats = []\n",
    "    mean_model_score = np.mean([i['Model_Score'] for i in models])\n",
    "    expert_pred_percentage = [i['Num_Experts_Predicted']/i['Num_Experts_Actual'] for i in models]\n",
    "    mn = np.array((1.0 - np.array(mean_model_score))) * np.array(models[0]['Total_Tested'])\n",
    "    stats = {'Mean_Wrong_Predictions' : round(mn),\n",
    "             'Mean_Model_Score' : mean_model_score, \n",
    "             'Expert_Prediction_Percentage' : expert_pred_percentage}\n",
    "    return stats\n",
    "\n",
    "def graph_from_statistics(stats):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(max_depth=3)\n",
    "\n",
    "display(run_model(clf_RF,X))\n",
    "display(run_model(clf_RF,X2))\n",
    "\n",
    "RF_bootstrap = bootstrap_model(clf_RF, X, 10)\n",
    "RF2_bootstrap = bootstrap_model(clf_RF, X2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a gaussian naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_NB = GaussianNB()\n",
    "\n",
    "display(run_model(clf_NB,X))\n",
    "display(run_model(clf_NB,X2))\n",
    "\n",
    "NB_bootstrap = bootstrap_model(clf_NB, X, 10)\n",
    "NB2_bootstrap = bootstrap_model(clf_NB, X2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_DT = tree.DecisionTreeClassifier()\n",
    "\n",
    "display(run_model(clf_DT, X))\n",
    "display(run_model(clf_DT, X2))\n",
    "\n",
    "DT_bootstrap = bootstrap_model(clf_DT, X, 10)\n",
    "DT2_bootstrap = bootstrap_model(clf_DT, X2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_stats = boot_statistics(DT_bootstrap)\n",
    "NB_stats = boot_statistics(NB_bootstrap)\n",
    "RF_stats = boot_statistics(RF_bootstrap)\n",
    "#\n",
    "DT2_stats = boot_statistics(DT2_bootstrap)\n",
    "NB2_stats = boot_statistics(NB2_bootstrap)\n",
    "RF2_stats = boot_statistics(RF2_bootstrap)\n",
    "\n",
    "display(DT_stats, DT2_stats)\n",
    "display(NB_stats, DT2_stats)\n",
    "display(RF_stats, RF2_stats)\n",
    "y = [DT_stats['Mean_Model_Score'],NB_stats['Mean_Model_Score'],RF_stats['Mean_Model_Score']]\n",
    "N = len(y)\n",
    "x = range(N)\n",
    "width = 1/1.5\n",
    "plt.bar(x, y, width)\n",
    "\n",
    "fig = plt.gcf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
