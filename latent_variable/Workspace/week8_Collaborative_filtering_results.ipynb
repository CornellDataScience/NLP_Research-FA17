{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a baseline for our method, that is, building a recommendation system without using text reviews. The method I use is collaborative filtering. \n",
    "In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).\n",
    "The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from __future__ import division\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_json_to_df(datapass):\n",
    "    data = [] \n",
    "    with open(datapass) as data_file: \n",
    "        for f in data_file:\n",
    "            data.append(json.loads(f))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = load_json_to_df(\"business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = load_json_to_df(\"review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = load_json_to_df(\"user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users_review = review.user_id.unique().shape[0]\n",
    "n_items_review = review.business_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Group res by city\n",
    "city = business.groupby('city')['city'].count()\n",
    "city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset restaurant to category restaurant\n",
    "#restaurant = business[business['categories'].str.contains(\"Restaurants\",na=False)]\n",
    "is_rest = []\n",
    "for i in business['categories']:\n",
    "    \n",
    "    if 'Restaurants' in i or 'Food' in i:\n",
    "        is_rest.append(True)\n",
    "    else:\n",
    "        is_rest.append(False)\n",
    "is_burger = []\n",
    "for i in business['categories']:\n",
    "    \n",
    "    if 'Burgers' in i:\n",
    "        is_burger.append(True)\n",
    "    else:\n",
    "        is_burger.append(False)\n",
    "\n",
    "is_m = []\n",
    "for i in business['categories']:\n",
    "    \n",
    "    if 'Michelin' in i:\n",
    "        is_m.append(True)\n",
    "    else:\n",
    "        is_m.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9616 E Independence Blvd</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...</td>\n",
       "      <td>SDMRxmcKPNt1AHPBKqO64Q</td>\n",
       "      <td>[Burgers, Bars, Restaurants, Sports Bars, Nigh...</td>\n",
       "      <td>Matthews</td>\n",
       "      <td>{'Monday': '11:00-0:00', 'Tuesday': '11:00-0:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.135196</td>\n",
       "      <td>-80.714683</td>\n",
       "      <td>Applebee's</td>\n",
       "      <td></td>\n",
       "      <td>28105</td>\n",
       "      <td>21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1794 Liverpool Road</td>\n",
       "      <td>{'RestaurantsTableService': True, 'GoodForMeal...</td>\n",
       "      <td>KW4y7uDGjVfU3ClkEjIGhg</td>\n",
       "      <td>[Burgers, Restaurants]</td>\n",
       "      <td>Pickering</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>43.834351</td>\n",
       "      <td>-79.090135</td>\n",
       "      <td>The Works</td>\n",
       "      <td></td>\n",
       "      <td>L1V 1V9</td>\n",
       "      <td>41</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     address  \\\n",
       "14  9616 E Independence Blvd   \n",
       "32       1794 Liverpool Road   \n",
       "\n",
       "                                           attributes             business_id  \\\n",
       "14  {'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...  SDMRxmcKPNt1AHPBKqO64Q   \n",
       "32  {'RestaurantsTableService': True, 'GoodForMeal...  KW4y7uDGjVfU3ClkEjIGhg   \n",
       "\n",
       "                                           categories       city  \\\n",
       "14  [Burgers, Bars, Restaurants, Sports Bars, Nigh...   Matthews   \n",
       "32                             [Burgers, Restaurants]  Pickering   \n",
       "\n",
       "                                                hours  is_open   latitude  \\\n",
       "14  {'Monday': '11:00-0:00', 'Tuesday': '11:00-0:0...        1  35.135196   \n",
       "32                                                 {}        1  43.834351   \n",
       "\n",
       "    longitude        name neighborhood postal_code  review_count  stars state  \n",
       "14 -80.714683  Applebee's                    28105            21    2.0    NC  \n",
       "32 -79.090135   The Works                  L1V 1V9            41    3.0    ON  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burger_res = business.loc[is_burger]\n",
    "burger_res.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"restaurants_pd_df.txt\", \"wb\") as f:\n",
    "    pickle.dump(restaurant, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "                 3\n",
       "110 Las Vegas    1\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city2 = restaurant.groupby('city')['city'].count()\n",
    "city.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"restaurants_pd_df.txt\", \"rb\") as f:\n",
    "    restaurant = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>3501 S Rainbow</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...</td>\n",
       "      <td>W-3Sy3fy85mQdd0ZNFKIiw</td>\n",
       "      <td>[Sports Bars, Nightlife, Burgers, Bars, Americ...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '11:00-0:00', 'Tuesday': '11:00-0:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.125270</td>\n",
       "      <td>-115.243588</td>\n",
       "      <td>Applebee's</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>89103</td>\n",
       "      <td>74</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>1021 S Buffalo Dr</td>\n",
       "      <td>{'GoodForMeal': {'dessert': False, 'latenight'...</td>\n",
       "      <td>q7OKOkEK-pgAQNjDiVd4bA</td>\n",
       "      <td>[American (Traditional), Restaurants, Burgers]</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>36.159785</td>\n",
       "      <td>-115.261803</td>\n",
       "      <td>Kilroy's</td>\n",
       "      <td>Westside</td>\n",
       "      <td>89162</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               address                                         attributes  \\\n",
       "302     3501 S Rainbow  {'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...   \n",
       "449  1021 S Buffalo Dr  {'GoodForMeal': {'dessert': False, 'latenight'...   \n",
       "\n",
       "                business_id  \\\n",
       "302  W-3Sy3fy85mQdd0ZNFKIiw   \n",
       "449  q7OKOkEK-pgAQNjDiVd4bA   \n",
       "\n",
       "                                            categories       city  \\\n",
       "302  [Sports Bars, Nightlife, Burgers, Bars, Americ...  Las Vegas   \n",
       "449     [American (Traditional), Restaurants, Burgers]  Las Vegas   \n",
       "\n",
       "                                                 hours  is_open   latitude  \\\n",
       "302  {'Monday': '11:00-0:00', 'Tuesday': '11:00-0:0...        1  36.125270   \n",
       "449                                                 {}        0  36.159785   \n",
       "\n",
       "      longitude        name neighborhood postal_code  review_count  stars  \\\n",
       "302 -115.243588  Applebee's    Chinatown       89103            74    2.5   \n",
       "449 -115.261803    Kilroy's     Westside       89162            12    3.0   \n",
       "\n",
       "    state  \n",
       "302    NV  \n",
       "449    NV  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#res_vegas = restaurant.loc[restaurant['city']=='Las Vegas']\n",
    "b_vegas = burger_res.loc[restaurant['city']=='Las Vegas']\n",
    "# with open(\"res_vegas_pd_df.txt\", \"wb\") as f:\n",
    "#     pickle.dump(res_vegas, f)\n",
    "b_vegas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505, 15)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_vegas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"res_vegas_pd_df.txt\", \"rb\") as f:\n",
    "    res_vegas = pickle.load(f)\n",
    "\n",
    "with open(\"restaurants_pd_df.txt\", \"rb\") as f:\n",
    "    restaurants = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset review and b_vegas to u,i,r\n",
    "review_ur = review[['user_id', 'stars','business_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>business_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cjpdDjZyprfyDG3RlkVG3w</td>\n",
       "      <td>5</td>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bjTcT8Ty4cJZhEOEo01FGA</td>\n",
       "      <td>3</td>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id  stars             business_id\n",
       "0  cjpdDjZyprfyDG3RlkVG3w      5  uYHaNptLzDLoV_JZ_MuzUA\n",
       "1  bjTcT8Ty4cJZhEOEo01FGA      3  uYHaNptLzDLoV_JZ_MuzUA"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_ur.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join restaruants in vegas table with review table\n",
    "review_rest_tor = pd.merge(b_vegas, review_ur, on='business_id', how='left')\n",
    "# review_rest_tor.shape\n",
    "# review_rest_tor.columns\n",
    "\n",
    "# Subset to user, item, rating columns\n",
    "uir = review_rest_tor[['user_id','business_id','stars_y']]\n",
    "\n",
    "with open('uir.txt', 'wb') as f:\n",
    "    pickle.dump(uir, f)\n",
    "# len(uir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"uir.txt\", \"rb\") as f:\n",
    "    uir =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign index for user and item\n",
    "user_index = uir.user_id.unique()\n",
    "item_index = uir.business_id.unique()\n",
    "\n",
    "# Count number of unique users and items\n",
    "n_users = uir.user_id.unique().shape[0]\n",
    "n_items = uir.business_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split User, Item, Rating dataset to train and test sets of 70% & 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(uir, test_size=0.30, random_state=42)\n",
    "# len(train)\n",
    "# len(test)\n",
    "\n",
    "# Create table for train data with list of users as index & items as columns\n",
    "train_matrix = pd.DataFrame(index=user_index, columns=item_index)\n",
    "# train_matrix.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in train_matrix table with ratings\n",
    "for row in train.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    train_matrix.loc[user][item] = row[3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create table for test data with list of users as index & items as columns    \n",
    "test_matrix = pd.DataFrame(index=user_index, columns=item_index)\n",
    "# test_matrix.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in test_matrix table with ratings\n",
    "for row in test.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    test_matrix.loc[user][item] = row[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin filtering process to create 5 Core Subset\n",
    "\n",
    "# Count number of rated items for each user\n",
    "item_1 = train_matrix.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_1.value_counts()\n",
    "\n",
    "# Filter down to the users with greater than or equal to 5 ratings\n",
    "train1 = train_matrix\n",
    "train1['item_1'] = item_1\n",
    "train2 = train1.loc[train1['item_1'] >= 5]\n",
    "# train2.shape\n",
    "\n",
    "# Count number of rated users for each item\n",
    "train2 = train2.drop('item_1',axis=1)\n",
    "train3 = train2.transpose()\n",
    "user_1 = train3.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_1.value_counts()\n",
    "\n",
    "# Filter down to the items with greater than or equal to 5 ratings\n",
    "train3['user_1'] = user_1\n",
    "train4 = train3.loc[train3['user_1'] >= 5]\n",
    "train4 = train4.drop('user_1',axis=1)\n",
    "train5 = train4.transpose()\n",
    "# train5.shape\n",
    "\n",
    "# Repeat the process for both user and item\n",
    "item_2 = train5.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_2.value_counts()\n",
    "# item_2.shape\n",
    "train5['item_2'] = item_2\n",
    "train6 = train5.loc[train5['item_2'] >= 5]\n",
    "train6 = train6.drop('item_2',axis=1)\n",
    "train7 = train6.transpose()\n",
    "user_2 = train7.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_2.value_counts()\n",
    "train7['user_2'] = user_2\n",
    "train8 = train7.loc[train7['user_2'] >= 5]\n",
    "train8 = train8.drop('user_2',axis=1)\n",
    "train9 = train8.transpose()\n",
    "# train9.shape\n",
    "\n",
    "# Check every user and item has at least 5 ratings\n",
    "item_3 = train9.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_3.value_counts()\n",
    "user_3 = train9.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_3.value_counts()\n",
    "\n",
    "# Filter down the test matrix to filtered user and item in train matrix\n",
    "test9 = test_matrix.loc[train9.index,train9.columns]\n",
    "# test9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train9.txt\", \"wb\") as f:\n",
    "    pickle.dump(train9, f)\n",
    "with open(\"test9.txt\", \"wb\") as f:\n",
    "    pickle.dump(test9, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_jI8TeypNwvXQDHM7Z8eA    2\n",
       "4fXZeX6b23YaAlhkkTldww    2\n",
       "en8pmJboMdEBRkXIIMsP4Q    2\n",
       "9YUrhoRIfXL4mVyPRNMgxQ    2\n",
       "qRYwodYPMMkl7QKnpmc39Q    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collaborative_filtering(train, test, sim='cosine', type='user', knn=5):\n",
    "\n",
    "    # Fill NAN values in train & test data as 0's\n",
    "    train_0 = train.fillna(0)\n",
    "    test_0 = test.fillna(0)\n",
    "\n",
    "    # Create a similarity matrix of either users or items based on cosine or \n",
    "    # pearson correlation measure\n",
    "    if sim == 'cosine':\n",
    "        user_dist = pairwise_distances(train_0, metric='cosine')\n",
    "        item_dist = pairwise_distances(train_0.T, metric='cosine')\n",
    "\n",
    "    elif sim == 'pearson':\n",
    "        user_dist = pairwise_distances(train_0, metric='correlation')\n",
    "        item_dist = pairwise_distances(train_0.T, metric='correlation')\n",
    "        \n",
    "    user_sim = 1 - user_dist\n",
    "    item_sim = 1 - item_dist\n",
    " \n",
    "    # Create a dataframe with mean user ratings\n",
    "    mean_rating = train.mean(axis=1)\n",
    "    mean_user_rating = pd.concat([mean_rating] * len(train.columns), axis=1)\n",
    "    mean_user_rating.columns = train.columns\n",
    "    \n",
    "    # Modify a dataframe so that mean user ratings are present only in matrix \n",
    "    # positions of rated items and 0's in matrix positions of non-rated items\n",
    "    mean_user_rating_0 = mean_user_rating\n",
    "    mean_user_rating_0[train_0 == 0] = 0\n",
    "    \n",
    "    # Normalize every user's ratings to mean of zero\n",
    "    ratings_diff = train_0 - mean_user_rating_0\n",
    "    \n",
    "    # Create a dataframe with user's mean user ratings present in all items\n",
    "    mean_user_rating_f = pd.concat([mean_rating] * len(train.columns), axis=1)\n",
    "    mean_user_rating_f.columns = train.columns\n",
    "    \n",
    "    # Create a placeholder dataframe for predictions of rated items in test data\n",
    "    pred = pd.DataFrame(index=train.index, columns=train.columns)\n",
    "    \n",
    "    # User-Based Collaborative Filtering\n",
    "    if type == 'user':    \n",
    "        # Index user similarity matrix with user ids for both rows and columns\n",
    "        user_sim = pd.DataFrame(user_sim, index=train.index, columns=train.index)\n",
    "        # When the number of k neareast neighbors is specified\n",
    "        if knn != 'all':\n",
    "            user_sim_mat = user_sim.as_matrix()\n",
    "        \n",
    "            # Item id's of rated items in test data for each user\n",
    "            cols = test.columns\n",
    "            test_rated = test.apply(lambda x: x > 0, raw=True).apply(lambda x: list(cols[x.values]), axis=1)\n",
    "            \n",
    "            # Iterate over each user, m\n",
    "            for m in range(len(user_sim)):\n",
    "                # Retrieve column of user m's similarities to all other users\n",
    "                temp = user_sim_mat[m]\n",
    "                temp = pd.DataFrame(temp,index=train.index, columns=['similarity'])\n",
    "                # Rank user m's similarities\n",
    "                temp['rank'] = temp['similarity'].rank(ascending=0)\n",
    "                \n",
    "                # Iterate over user m's rated items\n",
    "                for n in range(len(test_rated[m])):\n",
    "                    # For user m's nth rated item, extract column of ratings of \n",
    "                    # all users corresponding to nth item\n",
    "                    temp2 = ratings_diff[[test_rated[m][n]]]\n",
    "                    temp2.columns = ['rating']\n",
    "                    # Contatenate similarity, rank, rating as one dataframe\n",
    "                    result = pd.concat([temp, temp2], axis=1)    \n",
    "                    # Filter down to the users who rated the items\n",
    "                    result2 = result[result['rating'] != 0]\n",
    "                    # Filter down to knn number of users with the knn \n",
    "                    # highest similairites\n",
    "                    result3 = result2.nsmallest(int(knn), 'rank')\n",
    "                \n",
    "                    # Divide weighted sum of user's knn nearest neighbors' ratings by \n",
    "                    # sum of their similarities\n",
    "                    score = result3['similarity'].dot(result3['rating'])/result3['similarity'].sum()\n",
    "                    mean = mean_user_rating_f.loc[train.index[m]][test_rated[m][n]]\n",
    "                \n",
    "                    # Make a prediction by adding user's mean rating to weighted sum\n",
    "                    pred.loc[train.index[m]][test_rated[m][n]] = mean + score\n",
    "                    \n",
    "        # When the number of k neareast neighbors is not specified and all \n",
    "        # available neighbors are used for prediction     \n",
    "        elif knn == 'all':  \n",
    "            # Compute user similarity weighted sum of available ratings of \n",
    "            # user's every neighbor \n",
    "            num_user = user_sim.dot(ratings_diff)\n",
    "            \n",
    "            # Sum user similarities    \n",
    "            sum_sim_user = user_sim.sum(axis=1)\n",
    "            sum_sim_mat_user = pd.concat([sum_sim_user] * len(train.columns), axis=1)\n",
    "            sum_sim_mat_user.columns = train.columns\n",
    "\n",
    "            # Create a dataframe of predictions computed by adding mean user \n",
    "            # rating to user similairty weighted sum of user's ratings \n",
    "            # divided by sum of user similarities\n",
    "            pred = mean_user_rating_f + num_user / sum_sim_mat_user\n",
    "                \n",
    "    # Item-Based Collaborative Filtering  \n",
    "    elif type == 'item':\n",
    "        # Index item similarity matrix with item ids for both rows and columns\n",
    "        item_sim = pd.DataFrame(item_sim, index=train.columns, columns=train.columns)\n",
    "        \n",
    "        # When the number of k neareast neighbors is specified\n",
    "        if knn != 'all':\n",
    "            item_sim_mat = item_sim.as_matrix()\n",
    "\n",
    "            # User id's of rated items in test data for each item\n",
    "            cols2 = test.T.columns\n",
    "            test_rated2 = test.T.apply(lambda x: x > 0, raw=True).apply(lambda x: list(cols2[x.values]), axis=1)\n",
    "        \n",
    "            # Iterate over each item, m\n",
    "            for m in range(len(item_sim)):\n",
    "                # Retrieve column of item m's similarities to all other items\n",
    "                temp = item_sim_mat[m]\n",
    "                temp = pd.DataFrame(temp,index=train.columns, columns=['similarity'])\n",
    "                # Rank item m's similarities\n",
    "                temp['rank'] = temp['similarity'].rank(ascending=0)\n",
    "\n",
    "                # Iterate over item m's rated users\n",
    "                for n in range(len(test_rated2[m])):\n",
    "                    # For item m's nth rated user, extract column of ratings of \n",
    "                    # all items corresponding to nth user\n",
    "                    temp2 = ratings_diff.T[[test_rated2[m][n]]]\n",
    "                    temp2.columns = ['rating']\n",
    "                    # Contatenate similarity, rank, rating as one dataframe\n",
    "                    result = pd.concat([temp, temp2], axis=1)    \n",
    "                    # Filter down to the items that are rated by the user\n",
    "                    result2 = result[result['rating'] != 0]\n",
    "                    # Filter down to knn number of items with the knn \n",
    "                    # highest similairites\n",
    "                    result3 = result2.nsmallest(int(knn), 'rank')\n",
    "                \n",
    "                    # Divide weighted sum of item's knn nearest neighbors' \n",
    "                    # ratings by sum of their similarities\n",
    "                    score = result3['similarity'].dot(result3['rating'])/result3['similarity'].sum()\n",
    "                    mean = mean_user_rating_f.loc[test_rated2[m][n]][train.columns[m]]\n",
    "                \n",
    "                    # Make a prediction by adding user's mean rating to weighted sum\n",
    "                    pred.loc[test_rated2[m][n]][train.columns[m]] = mean + score\n",
    "        \n",
    "        # When the number of k neareast neighbors is not specified and \n",
    "        # all available neighbors are used for prediction\n",
    "        elif knn == 'all':\n",
    "            # Compute item similarity weighted sum of available ratings of \n",
    "            # item's every neighbor\n",
    "            num_item = ratings_diff.dot(item_sim)\n",
    "\n",
    "            # Sum item similarities \n",
    "            sum_sim_item = item_sim.sum(axis=1)\n",
    "            sum_sim_item = pd.DataFrame(sum_sim_item, index=train.columns)\n",
    "            sum_sim_mat_item = pd.concat([sum_sim_item.T] * len(train), axis=0)\n",
    "\n",
    "            sum_sim_mat_item.index = train.index\n",
    "\n",
    "            # Create a dataframe of predictions computed by adding mean user \n",
    "            # rating to item similairty weighted sum of items' ratings \n",
    "            # divided by sum of item similarities\n",
    "            pred = mean_user_rating_f + num_item / sum_sim_mat_item\n",
    "            \n",
    "    pred_0 = pred.fillna(0)\n",
    "    pred_0_mat = pred_0.as_matrix()            \n",
    "    return pred_0_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test9_0 = test9.fillna(0)\n",
    "test9_0_mat = test9_0.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mae(prediction, actual):\n",
    "    prediction = prediction[actual.nonzero()].flatten() \n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_absolute_error(prediction, actual)\n",
    "def mse(prediction, actual):\n",
    "    prediction = prediction[actual.nonzero()].flatten() \n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(prediction, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run American restaurants in Las Vegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuwenshen/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:76: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/xuwenshen/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:136: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Runs of Collaborative Filtering for Restaurant in Toronto took 57.63646197319031 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_user_5 = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='5')\n",
    "pred_user_10 = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='10')\n",
    "pred_user_20 = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='20')\n",
    "pred_user_all = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='all')\n",
    "pred_item_5 = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='5')\n",
    "pred_item_10 = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='10')\n",
    "pred_item_20 = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='20')\n",
    "pred_item_all = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='all')\n",
    "print ('8 Runs of Collaborative Filtering for Restaurant in Toronto took %s seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Runs of Collaborative Filtering for Restaurant in Toronto took 1.3039658069610596 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_user_cosine = collaborative_filtering(train9, test9, sim='cosine', type='user', knn='all')\n",
    "pred_user_pearson = collaborative_filtering(train9, test9, sim='pearson', type='user', knn='all')\n",
    "pred_item_cosine = collaborative_filtering(train9, test9, sim='cosine', type='item', knn='all')\n",
    "pred_item_pearson = collaborative_filtering(train9, test9, sim='pearson', type='item', knn='all')\n",
    "print ('4 Runs of Collaborative Filtering for Restaurant in Toronto took %s seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.2160378,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       ..., \n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_user_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Restaurnats in Las Vegas with Varying Number of Nearest Neighbors\n",
      "User-based CF using 5 nearest neighbors MAE: 3.3643\n",
      "User-based CF using 10 nearest neighbors MAE: 3.3639\n",
      "User-based CF using 20 nearest neighbors MAE: 3.362\n",
      "User-based CF using All available neighbors MAE: 0.9308\n",
      "Item-based CF using 5 nearest neighbors MAE: 3.3712\n",
      "Item-based CF using 10 nearest neighbors MAE: 3.369\n",
      "Item-based CF using 20 nearest neighbors MAE: 3.3686\n",
      "Item-based CF using All available neighbors MAE: 0.9361\n"
     ]
    }
   ],
   "source": [
    "print ('American Restaurnats in Las Vegas with Varying Number of Nearest Neighbors')\n",
    "print ('User-based CF using 5 nearest neighbors MAE: ' + str(round(mae(pred_user_5, test9_0_mat),4)))\n",
    "print ('User-based CF using 10 nearest neighbors MAE: ' + str(round(mae(pred_user_10, test9_0_mat),4)))\n",
    "print ('User-based CF using 20 nearest neighbors MAE: ' + str(round(mae(pred_user_20, test9_0_mat),4)))\n",
    "print ('User-based CF using All available neighbors MAE: ' + str(round(mae(pred_user_all, test9_0_mat),4)))\n",
    "print ('Item-based CF using 5 nearest neighbors MAE: ' + str(round(mae(pred_item_5, test9_0_mat),4)))\n",
    "print ('Item-based CF using 10 nearest neighbors MAE: ' + str(round(mae(pred_item_10, test9_0_mat),4)))\n",
    "print ('Item-based CF using 20 nearest neighbors MAE: ' + str(round(mae(pred_item_20, test9_0_mat),4)))\n",
    "print ('Item-based CF using All available neighbors MAE: ' + str(round(mae(pred_item_all, test9_0_mat),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Restaurnats in Las Vegas with Varying Number of Nearest Neighbors\n",
      "User-based CF using 5 nearest neighbors MSE: 13.0021\n",
      "User-based CF using 10 nearest neighbors MSE: 13.0036\n",
      "User-based CF using 20 nearest neighbors MSE: 13.0001\n",
      "User-based CF using All available neighbors MSE: 1.3475\n",
      "Item-based CF using 5 nearest neighbors MSE: 13.0236\n",
      "Item-based CF using 10 nearest neighbors MSE: 13.0189\n",
      "Item-based CF using 20 nearest neighbors MSE: 13.0179\n",
      "Item-based CF using All available neighbors MSE: 1.3597\n"
     ]
    }
   ],
   "source": [
    "print ('American Restaurnats in Las Vegas with Varying Number of Nearest Neighbors')\n",
    "print ('User-based CF using 5 nearest neighbors MSE: ' + str(round(mse(pred_user_5, test9_0_mat),4)))\n",
    "print ('User-based CF using 10 nearest neighbors MSE: ' + str(round(mse(pred_user_10, test9_0_mat),4)))\n",
    "print ('User-based CF using 20 nearest neighbors MSE: ' + str(round(mse(pred_user_20, test9_0_mat),4)))\n",
    "print ('User-based CF using All available neighbors MSE: ' + str(round(mse(pred_user_all, test9_0_mat),4)))\n",
    "print ('Item-based CF using 5 nearest neighbors MSE: ' + str(round(mse(pred_item_5, test9_0_mat),4)))\n",
    "print ('Item-based CF using 10 nearest neighbors MSE: ' + str(round(mse(pred_item_10, test9_0_mat),4)))\n",
    "print ('Item-based CF using 20 nearest neighbors MSE: ' + str(round(mse(pred_item_20, test9_0_mat),4)))\n",
    "print ('Item-based CF using All available neighbors MSE: ' + str(round(mse(pred_item_all, test9_0_mat),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# y = test9_0_mat\n",
    "# prediction = pred_item_all\n",
    "# ax.scatter(y, prediction, edgecolors=(0, 0, 0))\n",
    "# ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "# ax.set_xlabel('Expected')\n",
    "# ax.set_ylabel('Predicted')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.59091244,  4.59731854,  4.6       , ...,  4.60238919,\n",
       "         4.59806005,  4.59527225],\n",
       "       [ 2.90190875,  3.00679237,  3.        , ...,  2.99719351,\n",
       "         3.01004906,  2.99553396],\n",
       "       [ 4.34950552,  4.41355194,  4.42269869, ...,  4.40816916,\n",
       "         4.40131907,  4.37710422],\n",
       "       ..., \n",
       "       [ 3.49583794,  3.47771986,  3.49330436, ...,  3.51585984,\n",
       "         3.48989928,  3.5       ],\n",
       "       [ 3.80431016,  3.7990724 ,  3.80340129, ...,  3.80195253,\n",
       "         3.7997083 ,  3.80931724],\n",
       "       [ 3.79861863,  3.7938602 ,  3.79761898, ...,  3.7987674 ,\n",
       "         3.80881777,  3.80112589]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
