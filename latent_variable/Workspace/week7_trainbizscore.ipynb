{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_json_to_df(datapass):\n",
    "    data = [] \n",
    "    with open(datapass) as data_file: \n",
    "        for f in data_file:\n",
    "            data.append(json.loads(f))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = load_json_to_df(\"../../../dataset/business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = load_json_to_df(\"../../../dataset/review.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "Train the matrix of business subscores by minimizing a loss function for each restaurant, defined by sum((rec - rating)^2), where rec is (rating subscores dot user preference).\n",
    "\n",
    "We obtain user preference by running an LDA on all the text reviews, that is, we are setting the preference column as a constant in the loss function. \n",
    "\n",
    "The weight for each topic is calculated by normalizing the sum of the probabilty that each word of one user's texts occurs in the topics generated by LDA on all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>0</td>\n",
       "      <td>VfBHSwC5Vz_pbFluy07i9Q</td>\n",
       "      <td>5</td>\n",
       "      <td>My girlfriend and I stayed here for 3 nights a...</td>\n",
       "      <td>0</td>\n",
       "      <td>cjpdDjZyprfyDG3RlkVG3w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>0</td>\n",
       "      <td>3zRpneRKDsOPq92tq7ybAA</td>\n",
       "      <td>3</td>\n",
       "      <td>If you need an inexpensive place to stay for a...</td>\n",
       "      <td>0</td>\n",
       "      <td>bjTcT8Ty4cJZhEOEo01FGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>ne5WhI1jUFOcRn-b-gAzHA</td>\n",
       "      <td>3</td>\n",
       "      <td>Mittlerweile gibt es in Edinburgh zwei Ableger...</td>\n",
       "      <td>0</td>\n",
       "      <td>AXgRULmWcME7J6Ix3I--ww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>0</td>\n",
       "      <td>llmdwOgDReucVoWEry61Lw</td>\n",
       "      <td>4</td>\n",
       "      <td>Location is everything and this hotel has it! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>oU2SSOmsp_A8JYI7Z2JJ5w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uYHaNptLzDLoV_JZ_MuzUA</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>0</td>\n",
       "      <td>DuffS87NaSMDmIfluvT83g</td>\n",
       "      <td>5</td>\n",
       "      <td>gute lage im stadtzentrum. shoppingmeile und s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0xtbPEna2Kei11vsU-U2Mw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool        date  funny               review_id  \\\n",
       "0  uYHaNptLzDLoV_JZ_MuzUA     0  2016-07-12      0  VfBHSwC5Vz_pbFluy07i9Q   \n",
       "1  uYHaNptLzDLoV_JZ_MuzUA     0  2016-10-02      0  3zRpneRKDsOPq92tq7ybAA   \n",
       "2  uYHaNptLzDLoV_JZ_MuzUA     0  2015-09-17      0  ne5WhI1jUFOcRn-b-gAzHA   \n",
       "3  uYHaNptLzDLoV_JZ_MuzUA     0  2016-08-21      0  llmdwOgDReucVoWEry61Lw   \n",
       "4  uYHaNptLzDLoV_JZ_MuzUA     0  2013-11-20      0  DuffS87NaSMDmIfluvT83g   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      5  My girlfriend and I stayed here for 3 nights a...       0   \n",
       "1      3  If you need an inexpensive place to stay for a...       0   \n",
       "2      3  Mittlerweile gibt es in Edinburgh zwei Ableger...       0   \n",
       "3      4  Location is everything and this hotel has it! ...       0   \n",
       "4      5  gute lage im stadtzentrum. shoppingmeile und s...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  cjpdDjZyprfyDG3RlkVG3w  \n",
       "1  bjTcT8Ty4cJZhEOEo01FGA  \n",
       "2  AXgRULmWcME7J6Ix3I--ww  \n",
       "3  oU2SSOmsp_A8JYI7Z2JJ5w  \n",
       "4  0xtbPEna2Kei11vsU-U2Mw  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "0. Extract all the restaurant reviews.\n",
    "1. tokenize\n",
    "2. stop words\n",
    "3. stemmize\n",
    "4. get only nouns and adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65028, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_rest = []\n",
    "for i in business['categories']:\n",
    "    \n",
    "    if 'Restaurants' in i or 'Food' in i:\n",
    "        is_rest.append(True)\n",
    "    else:\n",
    "        is_rest.append(False)\n",
    "restaurants = business.loc[is_rest]\n",
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3216548, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_id = restaurants['business_id']\n",
    "rest_review = review.loc[review['business_id'].isin(rest_id)]\n",
    "rest_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rest_review.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupby_user = rest_review.groupby('user_id').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#groupby_user.sort_values('counts',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocess\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "en_stop = stopwords.words('english')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    raw = text.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    pospeech=[]\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    for j in tag:\n",
    "        if j[1] == 'NN' or j[1] == 'JJ':\n",
    "            pospeech.append(j[0])\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in pospeech if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    return stemmed_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating LDA \n",
    "For demo purpose, running LDA on only one user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 120\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "def getlda(df):\n",
    "    texts = []\n",
    "    for i in df['text']:\n",
    "        texts.append(preprocess(i))\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    # generate LDA model\n",
    "    ldamodel = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word = dictionary, passes=20)\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate preference maxtrix\n",
    "Add up the probabity that each word in the corpus in each topic, then add up for each topic and normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getprefer(userid, ldamodel, df):\n",
    "    user_reviews = df.loc[df['user_id'] == userid]\n",
    "    l = np.zeros(NUM_TOPICS)\n",
    "    texts = []\n",
    "    for t in user_reviews['text']:\n",
    "        texts.append(preprocess(t))\n",
    "    \n",
    "    for i in ldamodel.get_document_topics(corpus):\n",
    "        for topic in i:\n",
    "            l[topic[0]] += topic[1]\n",
    "    topic_likelihood = []\n",
    "    for i in l:\n",
    "        topic_likelihood.append(i/sum(l))\n",
    "    return topic_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize loss function\n",
    "<img src=\"formula.png\">\n",
    "Since we setting the topics for each user as constant, the corpus likelihood here is a constant so we only need to deal with rating error.\n",
    "\n",
    "We used Sequential Least SQuares Programming (SLSQP) to minize the function.\n",
    "\n",
    "Intial guess was randomized between 1.0 and 5.0\n",
    "\n",
    "Might need to explore bias and hyperparameters in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def min_loss(raw_prefer, actual_rating):\n",
    "    dim = NUM_TOPICS\n",
    "    prefer = []\n",
    "    for i in raw_prefer:\n",
    "        prefer.append(np.asarray(i, dtype=np.float32))\n",
    "    prefer = np.asarray(prefer)\n",
    "    print (prefer)\n",
    "    print (actual_rating)\n",
    "    bound = []\n",
    "    for i in range(0,dim):\n",
    "        bound.append((1.0,5.0))\n",
    "    bnds = tuple(bound)\n",
    "\n",
    "\n",
    "    def f(x,prefer,actual_rating):\n",
    "\n",
    "        return sum(abs(np.dot(x,np.transpose(prefer)) - actual_rating))\n",
    "    initial_guess = [np.random.uniform(1.0,5.0,dim)]\n",
    "    #initial_guess = np.full(5,2.5)\n",
    "    result = minimize(f, initial_guess, args=(prefer,actual_rating), method='SLSQP', bounds=bnds)\n",
    "    if result.success:\n",
    "        fitted_params = result.x\n",
    "        print(fitted_params)\n",
    "        print(result.fun)\n",
    "        print(result.nit)\n",
    "        return result\n",
    "    else:\n",
    "        raise ValueError(result.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine everything\n",
    "For each restaurant, run the algorithm to get the subscores, which will take a couple days on MacBook ~~Pro~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_prefer_to_df(df, lda, groupbyusers):\n",
    "    d = {}\n",
    "    new_df = df\n",
    "    for u in groupbyusers['user_id']:\n",
    "        d[u] = getprefer(u,lda,new_df)\n",
    "    df['preference'] = df.apply(lambda x: d[x['user_id']],axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_rest_subscore(bizid, df):\n",
    "    biz = df.loc[df['business_id'] == bizid]\n",
    "    rating = biz['stars']\n",
    "    preference = biz['preference']\n",
    "    result = min_loss(preference.values, rating.values)\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample = rest_review.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample_lda = getlda(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#groupby_user_sample = sample.groupby('user_id').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perfer_added_to_sample = add_prefer_to_df(sample, sample_lda, groupby_user_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perfer_added_to_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result_score = train_rest_subscore('HM0sWlwg3ONJ6syMSh_9zw', perfer_added_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#groupby_biz_sample = rest_review.groupby('business_id').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_subscore_to_df(df, groupbybiz):\n",
    "    d = {}\n",
    "    new_df = df\n",
    "    for u in groupbybiz['business_id']:\n",
    "        d[u] = train_rest_subscore(u, df)\n",
    "    df['subscore'] = df.apply(lambda x: d[x['business_id']],axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a4bdfae6f649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-da2df063974a>\u001b[0m in \u001b[0;36mgetlda\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# turn our tokenized documents into a id <-> term dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-cc2e759b1b88>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpospeech\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'JJ'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "lda = getlda(rest_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lda.pickle', 'wb') as handle:\n",
    "    pickle.dump(lda, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupby_user = rest_review.groupby('user_id').size().reset_index(name='counts')\n",
    "groupby_user.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefer_added = add_prefer_to_df(rest_review, lda, groupby_user)\n",
    "prefer_added.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscore_added = add_subscore_to_df(prefer_added, lda, group)\n",
    "subscore_added.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future\n",
    "Evaluating the performance.\n",
    "\n",
    "Come up with a method that does not set the topics as a whole but one for each user. However, we are struggling to see how to match topics between user and restaurant without manual labelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
