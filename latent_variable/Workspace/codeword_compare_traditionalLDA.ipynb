{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"documenttop1_text_list.txt\", \"rb\") as f:\n",
    "    document_top1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "neg = list(opinion_lexicon.negative())\n",
    "pos = list(opinion_lexicon.positive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert documents list to string\n",
    "doc = ''.join(document_top1)\n",
    "tokens = tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# stem token\n",
    "texts = [p_stemmer.stem(i) for i in stopped_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "neg = list(opinion_lexicon.negative())\n",
    "pos = list(opinion_lexicon.positive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def of adding codeword\n",
    "def codeword(words):\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in pos:\n",
    "            words.insert(i+1, \"GOODREVIEW\")\n",
    "        if words[i] in neg:\n",
    "            words.insert(i+1, \"NEGREVIEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeword(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chic',\n",
       " 'GOODREVIEW',\n",
       " 'although',\n",
       " 'menu',\n",
       " 'item',\n",
       " 'doesnt',\n",
       " 'scream',\n",
       " 'NEGREVIEW',\n",
       " 'french',\n",
       " 'cuisin',\n",
       " 'most',\n",
       " 'food',\n",
       " 'look',\n",
       " 'like',\n",
       " 'GOODREVIEW',\n",
       " 'can',\n",
       " 'get',\n",
       " 'american',\n",
       " 'place',\n",
       " 'the',\n",
       " 'food',\n",
       " 'awesom',\n",
       " 'though',\n",
       " 'one',\n",
       " 'place',\n",
       " 'I',\n",
       " 'actual',\n",
       " 'come',\n",
       " 'satisfi',\n",
       " 'I',\n",
       " 'order',\n",
       " 'smoke',\n",
       " 'NEGREVIEW',\n",
       " 'salmon',\n",
       " 'platter',\n",
       " 'I',\n",
       " 'enjoy',\n",
       " 'GOODREVIEW',\n",
       " 'I',\n",
       " 'enjoy',\n",
       " 'GOODREVIEW',\n",
       " 'simpl',\n",
       " 'food',\n",
       " 'My',\n",
       " 'plate',\n",
       " 'shred',\n",
       " 'salmon',\n",
       " 'pile',\n",
       " 'caper']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "no_features = 1000\n",
    "# # Use tf-idf features for NMF.\n",
    "# print(\"Extracting tf-idf features for NMF...\")\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=no_features,\n",
    "#                                    stop_words='english')\n",
    "# tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(texts)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "type(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topic words = 10\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "no_topics = 6\n",
    "\n",
    "# # Run NMF\n",
    "# nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# # Run LDA\n",
    "lda_codeword = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "place vega just bellagio love tri bread outsid frite amaz\n",
      "Topic 1:\n",
      "food order restaur come dinner gabi meal enjoy thi lunch\n",
      "Topic 2:\n",
      "servic time patio came breakfast ve perfect want onion salad\n",
      "Topic 3:\n",
      "negreview great steak like wait tabl ami strip mon recommend\n",
      "Topic 4:\n",
      "goodreview french fri right menu ask look better locat everyth\n",
      "Topic 5:\n",
      "good seat view realli delici nice fountain got reserv best\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_codeword, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, feature_names, no_top_words):\n",
    "    topic_lists = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list = []\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            topic_list += [feature_names[i]]\n",
    "        topic_lists.append(topic_list)\n",
    "    return topic_lists\n",
    "d = get_topics(lda_codeword, tf_feature_names, no_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "|   food   |  order  |  service  | bad price | good environment |  place   |\n",
      "+==========+=========+===========+===========+==================+==========+\n",
      "| place    | food    | servic    | negreview | goodreview       | good     |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| vega     | order   | time      | great     | french           | seat     |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| just     | restaur | patio     | steak     | fri              | view     |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| bellagio | come    | came      | like      | right            | realli   |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| love     | dinner  | breakfast | wait      | menu             | delici   |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| tri      | gabi    | ve        | tabl      | ask              | nice     |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| bread    | meal    | perfect   | ami       | look             | fountain |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| outsid   | enjoy   | want      | strip     | better           | got      |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| frite    | thi     | onion     | mon       | locat            | reserv   |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n",
      "| amaz     | lunch   | salad     | recommend | everyth          | best     |\n",
      "+----------+---------+-----------+-----------+------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "import texttable as tt\n",
    "tab = tt.Texttable()\n",
    "headings = ['food','order','service','bad price','good environment','place']\n",
    "tab.header(headings)\n",
    "d0 = d[0]\n",
    "d1 = d[1]\n",
    "d2 = d[2]\n",
    "d3 = d[3]\n",
    "d4 = d[4]\n",
    "d5 = d[5]\n",
    "# d6 = d[6]\n",
    "# d7 = d[7]\n",
    "# d8 = d[8]\n",
    "# d9 = d[9]\n",
    "\n",
    "\n",
    "for row in zip(d0,d1,d2,d3,d4,d5):\n",
    "    tab.add_row(row)\n",
    "\n",
    "s = tab.draw()\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "place vega just bellagio love tri bread outsid frite amaz peopl make soup experi tast butter friendli drink awesom disappoint\n",
      "Topic 1:\n",
      "food order restaur come dinner gabi meal enjoy thi lunch alway went dish star night don pari warm bit minut\n",
      "Topic 2:\n",
      "servic time patio came breakfast ve perfect want onion salad littl excel cook fresh water say thing start special way\n",
      "Topic 3:\n",
      "negreview great steak like wait tabl ami strip mon recommend chees watch server insid flavor la salmon reason filet scallop\n",
      "Topic 4:\n",
      "goodreview french fri right menu ask look better locat everyth appet baguett overal husband tender cut beauti room everyon yelp\n",
      "Topic 5:\n",
      "good seat view realli delici nice fountain got reserv best sit price eat sauc egg definit friend brunch wine didn\n"
     ]
    }
   ],
   "source": [
    "# topic words = 20\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 20\n",
    "no_topics = 6\n",
    "\n",
    "# # Run NMF\n",
    "# nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# # Run LDA\n",
    "lda_codeword = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "display_topics(lda_codeword, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "|    food    |  order  |  service  | bad price | good environment |  place   |\n",
      "+============+=========+===========+===========+==================+==========+\n",
      "| place      | food    | servic    | negreview | goodreview       | good     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| vega       | order   | time      | great     | french           | seat     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| just       | restaur | patio     | steak     | fri              | view     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| bellagio   | come    | came      | like      | right            | realli   |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| love       | dinner  | breakfast | wait      | menu             | delici   |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| tri        | gabi    | ve        | tabl      | ask              | nice     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| bread      | meal    | perfect   | ami       | look             | fountain |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| outsid     | enjoy   | want      | strip     | better           | got      |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| frite      | thi     | onion     | mon       | locat            | reserv   |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| amaz       | lunch   | salad     | recommend | everyth          | best     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| peopl      | alway   | littl     | chees     | appet            | sit      |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| make       | went    | excel     | watch     | baguett          | price    |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| soup       | dish    | cook      | server    | overal           | eat      |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| experi     | star    | fresh     | insid     | husband          | sauc     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| tast       | night   | water     | flavor    | tender           | egg      |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| butter     | don     | say       | la        | cut              | definit  |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| friendli   | pari    | thing     | salmon    | beauti           | friend   |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| drink      | warm    | start     | reason    | room             | brunch   |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| awesom     | bit     | special   | filet     | everyon          | wine     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n",
      "| disappoint | minut   | way       | scallop   | yelp             | didn     |\n",
      "+------------+---------+-----------+-----------+------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# table of 6 topics with 20 words from CODEWORD LDA\n",
    "def get_topics(model, feature_names, no_top_words):\n",
    "    topic_lists = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list = []\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            topic_list += [feature_names[i]]\n",
    "        topic_lists.append(topic_list)\n",
    "    return topic_lists\n",
    "d = get_topics(lda_codeword, tf_feature_names, no_top_words)\n",
    "import texttable as tt\n",
    "tab = tt.Texttable()\n",
    "headings = ['food','order','service','bad price','good environment','place']\n",
    "tab.header(headings)\n",
    "d0 = d[0]\n",
    "d1 = d[1]\n",
    "d2 = d[2]\n",
    "d3 = d[3]\n",
    "d4 = d[4]\n",
    "d5 = d[5]\n",
    "for row in zip(d0,d1,d2,d3,d4,d5):\n",
    "    tab.add_row(row)\n",
    "\n",
    "s = tab.draw()\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "food just love tri mon amaz experi ask say baguett sandwich awesom said potato entre howev seafood qualiti super waffl\n",
      "Topic 1:\n",
      "steak order restaur come bread ami soup star pari minut chicken benedict perfectli attent cream beauti absolut hour larg cours\n",
      "Topic 2:\n",
      "servic patio fountain frite make want onion salad butter cook look water warm start toast trip mignon busi coffe mani\n",
      "Topic 3:\n",
      "negreview great like price chees insid better scallop escargot special sure know need waitress crispi light quit share anoth ate\n",
      "Topic 4:\n",
      "goodreview menu room face platter doesn simpl begin air wa crusti play alright mostli elsewher horribl mood caus 18 shade\n",
      "Topic 5:\n",
      "view realli nice outsid got best sit eat definit brunch wine didn day pretti dine crepe waiter wasn think outdoor\n",
      "Topic 6:\n",
      "bellagio tabl dinner strip peopl recommend server lunch flavor salmon locat favorit worth everi disappoint walk fantast portion hotel parti\n",
      "Topic 7:\n",
      "vega time wait sauc enjoy littl alway went night filet everyth sinc way staff bad feel ambianc husband medium sweet\n",
      "Topic 8:\n",
      "good seat came reserv breakfast ve egg fri right serv tast thing appet dessert wonder overal sat end someth free\n",
      "Topic 9:\n",
      "place french delici gabi meal perfect thi friend watch excel dish don fresh bit la reason visit friendli expect drink\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# topic words = 20 topic = 10\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 20\n",
    "no_topics = 10\n",
    "\n",
    "# # Run NMF\n",
    "# nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# # Run LDA\n",
    "lda_codeword = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "display_topics(lda_codeword, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "|   food   |   order   | service  | bad price | good environment |  place  |\n",
      "+==========+===========+==========+===========+==================+=========+\n",
      "| food     | steak     | servic   | negreview | goodreview       | view    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| just     | order     | patio    | great     | menu             | realli  |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| love     | restaur   | fountain | like      | room             | nice    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| tri      | come      | frite    | price     | face             | outsid  |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| mon      | bread     | make     | chees     | platter          | got     |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| amaz     | ami       | want     | insid     | doesn            | best    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| experi   | soup      | onion    | better    | simpl            | sit     |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| ask      | star      | salad    | scallop   | begin            | eat     |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| say      | pari      | butter   | escargot  | air              | definit |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| baguett  | minut     | cook     | special   | wa               | brunch  |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| sandwich | chicken   | look     | sure      | crusti           | wine    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| awesom   | benedict  | water    | know      | play             | didn    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| said     | perfectli | warm     | need      | alright          | day     |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| potato   | attent    | start    | waitress  | mostli           | pretti  |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| entre    | cream     | toast    | crispi    | elsewher         | dine    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| howev    | beauti    | trip     | light     | horribl          | crepe   |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| seafood  | absolut   | mignon   | quit      | mood             | waiter  |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| qualiti  | hour      | busi     | share     | caus             | wasn    |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| super    | larg      | coffe    | anoth     | 18               | think   |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n",
      "| waffl    | cours     | mani     | ate       | shade            | outdoor |\n",
      "+----------+-----------+----------+-----------+------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# table of 10 topic with 20 words from CODEWORD LDA\n",
    "def get_topics(model, feature_names, no_top_words):\n",
    "    topic_lists = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list = []\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            topic_list += [feature_names[i]]\n",
    "        topic_lists.append(topic_list)\n",
    "    return topic_lists\n",
    "d = get_topics(lda_codeword, tf_feature_names, no_top_words)\n",
    "import texttable as tt\n",
    "tab = tt.Texttable()\n",
    "headings = ['food','order','service','bad price','good environment','place']\n",
    "tab.header(headings)\n",
    "d0 = d[0]\n",
    "d1 = d[1]\n",
    "d2 = d[2]\n",
    "d3 = d[3]\n",
    "d4 = d[4]\n",
    "d5 = d[5]\n",
    "for row in zip(d0,d1,d2,d3,d4,d5):\n",
    "    tab.add_row(row)\n",
    "\n",
    "s = tab.draw()\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "salad medium rare burger bacon wow tuna old egg kale\n",
      "Topic 1:\n",
      "great food vegas bellagio place service patio view strip outside\n",
      "Topic 2:\n",
      "steak good ordered soup frites bread french onion cheese fries\n",
      "Topic 3:\n",
      "breakfast eggs crepe brunch benedict toast good coffee french ordered\n",
      "Topic 4:\n",
      "food service good table just time wait great place restaurant\n",
      "Topic 5:\n",
      "mon ami gabi good french great just wine vegas filet\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "|  food  |  order   | service |   price   | environment | place  |\n",
      "+========+==========+=========+===========+=============+========+\n",
      "| salad  | great    | steak   | breakfast | food        | mon    |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| medium | food     | good    | eggs      | service     | ami    |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| rare   | vegas    | ordered | crepe     | good        | gabi   |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| burger | bellagio | soup    | brunch    | table       | good   |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| bacon  | place    | frites  | benedict  | just        | french |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| wow    | service  | bread   | toast     | time        | great  |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| tuna   | patio    | french  | good      | wait        | just   |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| old    | view     | onion   | coffee    | great       | wine   |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| egg    | strip    | cheese  | french    | place       | vegas  |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n",
      "| kale   | outside  | fries   | ordered   | restaurant  | filet  |\n",
      "+--------+----------+---------+-----------+-------------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Compare with traditional LDA\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(document_top1)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "no_topics = 6\n",
    "no_top_words = 10\n",
    "lda_codeword = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "# topic words = 10\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "display_topics(lda_codeword, tf_feature_names, no_top_words)\n",
    "def get_topics(model, feature_names, no_top_words):\n",
    "    topic_lists = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list = []\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            topic_list += [feature_names[i]]\n",
    "        topic_lists.append(topic_list)\n",
    "    return topic_lists\n",
    "d = get_topics(lda_codeword, tf_feature_names, no_top_words)\n",
    "import texttable as tt\n",
    "tab = tt.Texttable()\n",
    "headings = ['food','order','service','price','environment','place']\n",
    "tab.header(headings)\n",
    "d0 = d[0]\n",
    "d1 = d[1]\n",
    "d2 = d[2]\n",
    "d3 = d[3]\n",
    "d4 = d[4]\n",
    "d5 = d[5]\n",
    "for row in zip(d0,d1,d2,d3,d4,d5):\n",
    "    tab.add_row(row)\n",
    "\n",
    "s = tab.draw()\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
