{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from text_embedder import TextEmbedder\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(stars):\n",
    "    '''\n",
    "    build one hot encoding for the star rating\n",
    "    '''\n",
    "    res = []\n",
    "    for s in stars:\n",
    "        out = np.array([0.0]*5)\n",
    "        out[s-1] = 1.0\n",
    "        res.append(out)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_three(stars):\n",
    "    res = []\n",
    "    for s in stars:\n",
    "        out = np.array([0.0]*3)\n",
    "        if s == 5:\n",
    "            out[2] = 1.0\n",
    "        elif s==1:\n",
    "            out[0] = 1.0\n",
    "        else:\n",
    "            out[1] = 1.0\n",
    "        res.append(out)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input_data, size_in, size_out, name):\n",
    "    '''\n",
    "    Implement tensor\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # weight as random normal variables\n",
    "        w = tf.Variable(tf.random_normal([size_in, size_out]), name = 'W')\n",
    "        # bias as random normal variables\n",
    "        b = tf.Variable(tf.random_normal([size_out]), name = 'B')\n",
    "        activation = tf.atan(tf.matmul(input_data, w) + b)\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_layer(input_data, size_in, size_out, name):\n",
    "    '''\n",
    "    output tensor\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # weight as random normal variables\n",
    "        w = tf.Variable(tf.random_normal([size_in, size_out]), name = 'W')\n",
    "        # bias as random normal variables\n",
    "        b = tf.Variable(tf.random_normal([size_out]), name = 'B')\n",
    "\n",
    "        activation = tf.atan(tf.matmul(input_data, w) + b)\n",
    "\n",
    "        return activation, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x,input_size, hidden, out_size, gate_dimention = 0.0, drop_out = 1.0):\n",
    "    '''\n",
    "    implement a filter gate before input\n",
    "    '''\n",
    "    # implement random filter here\n",
    "    \n",
    "    prev = input_size\n",
    "    if gate_dimention != 0.0:\n",
    "        gate = np.zeros(input_size)\n",
    "        i = random.sample(set(np.arange(input_size)), gate_dimention)\n",
    "        gate[[i]] = 1.0\n",
    "        activation = tf.multiply(x, gate)\n",
    "    else:\n",
    "        activation = x\n",
    "        \n",
    "    # add dropout layer with specified probability\n",
    "    if drop_out != 1.0:\n",
    "        activation = tf.nn.dropout(x, drop_out)\n",
    "\n",
    "    # build a series of hidden layers\n",
    "    for name,i in enumerate(hidden):\n",
    "        activation = layer(activation, prev, i, 'hiddenlayer-'+str(name))\n",
    "        prev = i\n",
    "\n",
    "    # build an output layer\n",
    "    embedding_in = activation\n",
    "    if out_size == 1:\n",
    "        out, weights = output_layer_sm(activation, hidden[-1], out_size, 'output')\n",
    "    else:\n",
    "        out, weights = output_layer(activation, hidden[-1], out_size, 'output')\n",
    "\n",
    "    return out, embedding_in, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_embedding(enum, df, embedder):\n",
    "    out = []\n",
    "    if enum == 0:\n",
    "        for d in df['text'].values:\n",
    "            out.append(embedder.augmented_embed_text(d))\n",
    "    elif enum == 1:\n",
    "        for d,u in zip(df['text'].values, df['user_id'].values):\n",
    "            out.append(embedder.user_tfidf_embed(d, u))\n",
    "    elif enum == 2:\n",
    "        for d,b in zip(df['text'].values, df['business_id'].values):\n",
    "            out.append(embedder.user_tf_business_idf(d, b))\n",
    "    elif enum == 3:\n",
    "        for d, u, b in zip(df['text'].values, df['user_id'].values, df['business_id'].values):\n",
    "            out.append(embedder.user_tfidf_business_idf(d, u, b))\n",
    "    else:\n",
    "        print ('invalid enum')\n",
    "        return None\n",
    "\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained data\n",
    "business = pd.read_csv('../data/chinese_business_clean.csv')\n",
    "reviews = pd.read_csv('../data/chinese_reviews_clean.csv')\n",
    "\n",
    "lda =  models.LdaModel.load('../data/gensim/lda.model')\n",
    "dictionary = corpora.Dictionary.load('../data/gensim/chinsese_dict.dict')\n",
    "\n",
    "with open('../data/u_idf.pickle', 'rb') as f:\n",
    "    uidf_data = pickle.load(f)\n",
    "\n",
    "with open('../data/b_idf.pickle', 'rb') as f:\n",
    "    bidf_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedder = TextEmbedder(model = lda, dictionary = dictionary, user_idf = uidf_data, business_idf = bidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>yfxDa8RFOvJPQh0rNtakHA</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>pH0BLkL4cbxKzu471VZnuA</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>X8c23dur0ll2D9XTu-I8Qg</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "      <td>1525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>GJ_bXUPv672YwNg4TneJog</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "      <td>1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>cHdJXLlKNWixBXpDwEGb_A</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "      <td>1246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id  cool  date  funny  review_id  stars  text  \\\n",
       "3692  yfxDa8RFOvJPQh0rNtakHA  2380  2380   2380       2380   2380  2380   \n",
       "3088  pH0BLkL4cbxKzu471VZnuA  1948  1948   1948       1948   1948  1948   \n",
       "1944  X8c23dur0ll2D9XTu-I8Qg  1525  1525   1525       1525   1525  1525   \n",
       "953   GJ_bXUPv672YwNg4TneJog  1302  1302   1302       1302   1302  1302   \n",
       "2312  cHdJXLlKNWixBXpDwEGb_A  1246  1246   1246       1246   1246  1246   \n",
       "\n",
       "      useful  user_id  \n",
       "3692    2380     2380  \n",
       "3088    1948     1948  \n",
       "1944    1525     1525  \n",
       "953     1302     1302  \n",
       "2312    1246     1246  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.groupby('business_id').count().reset_index().sort_values(by = 'review_id', ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# focus on data for each business\n",
    "case_review1 = reviews[reviews['business_id'] == 'yfxDa8RFOvJPQh0rNtakHA']\n",
    "case_review2 = reviews[reviews['business_id'] == 'pH0BLkL4cbxKzu471VZnuA']\n",
    "case_review3 = reviews[reviews['business_id'] == 'X8c23dur0ll2D9XTu-I8Qg']\n",
    "case_review4 = reviews[reviews['business_id'] == 'GJ_bXUPv672YwNg4TneJog']\n",
    "case_review5 = reviews[reviews['business_id'] == 'cHdJXLlKNWixBXpDwEGb_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, training_epoch, embedder, enum, beta = 0.0, gate_size = 0.0, drop_out = 1.0, learning_rate = 0.01, hidden_layer = [100, 80], out_layer = 5):\n",
    "    id_train, id_test, star_train, star_test = train_test_split(data, data['stars'], test_size=0.2)\n",
    "\n",
    "    # generate labels\n",
    "    if out_layer == 5:\n",
    "        one_hot_star = one_hot(star_train)\n",
    "        one_hot_star_test = one_hot(star_test)\n",
    "    elif out_layer == 3:\n",
    "        one_hot_star = one_hot_three(star_train)\n",
    "        one_hot_star_test = one_hot_three(star_test)\n",
    "    elif out_layer == 1:\n",
    "        one_hot_star = one_hot_soft_max(star_train)\n",
    "        one_hot_star_test = one_hot_soft_max(star_test)\n",
    "    \n",
    "    print ('embedding dataset ...')\n",
    "    # validation set\n",
    "    embed_in = gen_embedding(enum, id_train, embedder)\n",
    "    \n",
    "    # test set \n",
    "    embed_test = gen_embedding(enum, id_test, embedder)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape = [None, 128], name = 'input_topic') # number of topics\n",
    "    y = tf.placeholder(tf.float32, shape = [None, out_layer], name = 'softmax') # 5 stars\n",
    "\n",
    "    embedded_size = hidden_layer[-1]\n",
    "    out, embedding_in, weights = build_model(x, 128, hidden_layer, out_layer, gate_size, drop_out) # shape of (?, 5)\n",
    "    \n",
    "    \n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        if out_layer == 1:\n",
    "            cross_entropy = tf.multiply(tf.reduce_mean(tf.pow(out - y, 2)), 10)\n",
    "        else:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= out, labels = y))\n",
    "            regularizer = tf.nn.l2_loss(weights)\n",
    "            cross_entropy = tf.reduce_mean(cross_entropy + beta * regularizer)\n",
    "    # optimization\n",
    "    with tf.name_scope(\"train\"):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "    # reports\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        if out_layer == 1:\n",
    "            correct = tf.reduce_mean(tf.cast(cross_entropy , dtype = tf.float32))\n",
    "        else:\n",
    "            correct = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(y, 1)), dtype = tf.float32))\n",
    "    \n",
    "    print ('training starts ...')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(training_epoch):\n",
    "            idx = random.sample(set(np.arange(len(id_train))), 10)\n",
    "            # create embedding with embedder\n",
    "            x_in = embed_in[idx]\n",
    "            y_out = one_hot_star[idx]\n",
    "            \n",
    "            if epoch % 10000 == 0:\n",
    "                [accuracy] = sess.run([correct], feed_dict = {x:embed_in[:10], y:one_hot_star[:10]})\n",
    "                print ('%.4f' % accuracy)\n",
    "            sess.run(opt, feed_dict = {x:x_in, y:y_out})\n",
    "\n",
    "        pred = tf.nn.softmax(out)  # Apply softmax to logits\n",
    "        # Calculate accuracy\n",
    "        print(\"Accuracy:\", sess.run(correct, feed_dict = {x: embed_test, y: one_hot_star_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.10\n",
      "0.30\n",
      "0.10\n",
      "0.30\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.30\n",
      "0.30\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "0.20\n",
      "Accuracy: 0.35084\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 2000000, embedder, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.10\n",
      "0.40\n",
      "0.50\n",
      "0.70\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "0.90\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "0.90\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "Accuracy: 0.355042\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.00\n",
      "0.40\n",
      "0.40\n",
      "0.00\n",
      "0.00\n",
      "0.40\n",
      "0.30\n",
      "0.40\n",
      "0.00\n",
      "0.10\n",
      "0.30\n",
      "0.40\n",
      "0.40\n",
      "0.40\n",
      "0.40\n",
      "0.30\n",
      "0.40\n",
      "0.40\n",
      "0.10\n",
      "0.10\n",
      "Accuracy: 0.405462\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 2000000, embedder, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.30\n",
      "0.30\n",
      "0.80\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.80\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "Accuracy: 0.371849\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.30\n",
      "0.10\n",
      "0.20\n",
      "0.30\n",
      "0.30\n",
      "0.40\n",
      "0.20\n",
      "0.20\n",
      "0.30\n",
      "0.20\n",
      "0.50\n",
      "0.50\n",
      "0.30\n",
      "0.60\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.60\n",
      "Accuracy: 0.380252\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 3, drop_out = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.00\n",
      "0.60\n",
      "0.70\n",
      "0.80\n",
      "0.90\n",
      "0.90\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "0.70\n",
      "1.00\n",
      "0.90\n",
      "1.00\n",
      "1.00\n",
      "1.00\n",
      "0.90\n",
      "1.00\n",
      "0.90\n",
      "Accuracy: 0.32563\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 3, drop_out = 0.9, beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.20\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "0.10\n",
      "Accuracy: 0.157563\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 3, drop_out = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.00\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.10\n",
      "0.90\n",
      "1.00\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.90\n",
      "0.10\n",
      "0.90\n",
      "0.90\n",
      "0.10\n",
      "0.90\n",
      "0.90\n",
      "Accuracy: 0.668067\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 2, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.10\n",
      "0.60\n",
      "0.20\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.20\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "Accuracy: 0.684874\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 2, drop_out = 0.9, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.30\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.70\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.40\n",
      "0.30\n",
      "0.30\n",
      "0.60\n",
      "0.30\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.30\n",
      "0.60\n",
      "0.30\n",
      "0.60\n",
      "0.30\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.30\n",
      "0.30\n",
      "0.60\n",
      "0.60\n",
      "0.60\n",
      "0.40\n",
      "Accuracy: 0.665966\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 500000, embedder, 2, drop_out = 0.9, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "0.70\n",
      "Accuracy: 0.64916\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 2, drop_out = 0.8, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "0.1000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.7000\n",
      "0.4000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "0.8000\n",
      "Accuracy: 0.668067\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 200000, embedder, 2, drop_out = 0.8, hidden_layer = [120, 100], out_layer = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Embedding Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 87 ms, total: 22.8 s\n",
      "Wall time: 22.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00800386,  0.00751197,  0.00751197, ...,  0.00751197,\n",
       "         0.00751197,  0.00751197],\n",
       "       [ 0.0074958 ,  0.0074958 ,  0.0074958 , ...,  0.0074958 ,\n",
       "         0.0074958 ,  0.0074958 ],\n",
       "       [ 0.00749794,  0.00749794,  0.00749794, ...,  0.00749794,\n",
       "         0.00864102,  0.00749794],\n",
       "       ..., \n",
       "       [ 0.00755739,  0.00755739,  0.00755739, ...,  0.00755739,\n",
       "         0.00755739,  0.00755739],\n",
       "       [ 0.00755062,  0.00755062,  0.00755062, ...,  0.00755062,\n",
       "         0.00755062,  0.00755062],\n",
       "       [ 0.00746532,  0.00746532,  0.00746532, ...,  0.00746532,\n",
       "         0.00746532,  0.00746532]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gen_embedding(0, d_train, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 78.9 ms, total: 23 s\n",
      "Wall time: 23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04904512,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.01719956,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.02732329, ...,  0.        ,\n",
       "         0.        ,  0.02998012],\n",
       "       [ 0.        ,  0.        ,  0.02121954, ...,  0.02121954,\n",
       "         0.02121954,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01717248, ...,  0.02694746,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gen_embedding(1, d_train, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 90.8 ms, total: 23.1 s\n",
      "Wall time: 23.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00847174,  0.00901358,  0.00538159, ...,  0.00750334,\n",
       "         0.00979433,  0.00845681],\n",
       "       [ 0.00799792,  0.00891608,  0.00532337, ...,  0.00742217,\n",
       "         0.00968838,  0.00836532],\n",
       "       [ 0.00796717,  0.0088818 ,  0.00530291, ...,  0.00739364,\n",
       "         0.00965114,  0.00833316],\n",
       "       ..., \n",
       "       [ 0.00790171,  0.00880883,  0.00525934, ...,  0.00733289,\n",
       "         0.00957184,  0.00914302],\n",
       "       [ 0.00805194,  0.00897631,  0.00535933, ...,  0.00747231,\n",
       "         0.00975383,  0.00842183],\n",
       "       [ 0.00796804,  0.00888277,  0.00530348, ...,  0.00832823,\n",
       "         0.00965219,  0.00833407]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gen_embedding(2, d_train, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 86.3 ms, total: 23 s\n",
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06876797,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.01900364,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.02111689, ...,  0.        ,\n",
       "         0.        ,  0.03653553],\n",
       "       [ 0.        ,  0.        ,  0.01671909, ...,  0.02331078,\n",
       "         0.03042826,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01375876, ...,  0.0279899 ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gen_embedding(3, d_train, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try with Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_layer_sm(input_data, size_in, size_out, name):\n",
    "    '''\n",
    "    output tensor\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # weight as random normal variables\n",
    "        w = tf.Variable(tf.random_normal([size_in, size_out]), name = 'W')\n",
    "        # bias as random normal variables\n",
    "        b = tf.Variable(tf.random_normal([size_out]), name = 'B')\n",
    "        \n",
    "        activation = tf.nn.softmax(tf.atan(tf.matmul(input_data, w) + b, name = 'soft_max'))\n",
    "\n",
    "        return activation, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_soft_max(stars):\n",
    "    res = []\n",
    "    for s in stars:\n",
    "        out = np.array([1.0 * s/5.0])\n",
    "        res.append(out)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n"
     ]
    }
   ],
   "source": [
    "train(case_review1, 100000, embedder, 2, drop_out = 0.9, out_layer = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dataset ...\n",
      "training starts ...\n",
      "1.9600\n",
      "1.9600\n",
      "1.9600\n",
      "1.9600\n",
      "1.9600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8f19b47e7259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_review1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6c3e41a06057>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, training_epoch, embedder, enum, beta, gate_size, drop_out, learning_rate, hidden_layer, out_layer)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0membed_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mone_hot_star\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply softmax to logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(case_review1, 50000, embedder, 2, drop_out = 0.8, hidden_layer = [120, 100], out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
