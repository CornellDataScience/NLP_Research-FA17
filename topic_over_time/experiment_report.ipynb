{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from lda_embedder.text_embedder import TextEmbedder\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Sequencial Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that Yelpâ€™s star rating system does not reflect time-sensitive information of business performance (whether or not the business is improving at given time), which the users generally care more about. In addition to establishing a metric that represents temporary performance, we wanted to analyze why the business is undergoing a successful period and if not, suggest how to improve them. For this reason, we could not simply take the average of most recent N reviews, but rather, needed to investigate the contents of review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose 3 different embeddings, which is our implementations of modified LDA. We used LDA for following reasons:  \n",
    "- We can easily map texts to vetcor, unlike word2vec, which maps word to vector\n",
    "- High interpretability\n",
    "- Adjustable based on time or based on business. (Our embedding for the identical raw texts will look different if the business is different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load file\n",
    "business = pd.read_csv('data/chinese_business_clean.csv')\n",
    "reviews = pd.read_csv('data/chinese_reviews_clean_offsets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pretrained topic models\n",
    "lda =  models.LdaModel.load('data/gensim/lda.model')\n",
    "dictionary = corpora.Dictionary.load('data/gensim/chinsese_dict.dict')\n",
    "user1 = reviews[reviews['user_id'] == 'CxDOIDnH8gp9KXzpBHJYXw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This place is horrible, we were so excited to try it since I got a gift card for my birthday. We went in an ordered are whole meal and they did not except are gift card, because their system was down. Unacceptable, this would have been so helpful if we would have known this prior!!',\n",
       " 'jQsNFOzDpxPmOurSWCg1vQ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this sample to embed\n",
    "sample = reviews['text'].values[0]\n",
    "reviews['text'].values[0], reviews['business_id'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. embed (not using this)\n",
    "Simply uses gensim built-in get_document_topics function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed(text, model, dictionary):\n",
    "    text = tokenize(text)\n",
    "    bow = dictionary.doc2bow(text)\n",
    "    kindex = model.get_document_topics(bow)\n",
    "    out = [0] * model.num_topics\n",
    "    for i, p in kindex:\n",
    "        out[i] = p\n",
    "    return np.array(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.94487847,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(sample, lda, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. embed_sent (baseline)\n",
    "\n",
    "Apply Dirichlet distribution after tokenize by sentences.\n",
    "\n",
    "$\\theta_{d} = \\frac{1}{n} \\sum^{n}_{s \\in d} Dirishlet(\\alpha_{s})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_sent(text, model, dictionary, sent_length = False):\n",
    "    out = np.array([0.]*128)\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    for text in nltk.sent_tokenize(text):\n",
    "        out += embed(text, lda, dictionary)\n",
    "    if sent_length:\n",
    "        return out/sentences, sentences\n",
    "    return (out/sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.59200754,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.0398718 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sent(sample, lda, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. augmented embed  \n",
    "The baseline embedding prones to leave most entries 0. This caused overfitting for the later evaluation. We needed to scale and smooth this value by certain parameter.\n",
    "\n",
    "$\\theta_{d}(\\beta) = \\beta + \\beta * \\frac{1}{n} \\frac{\\sum_{s \\in d} ^ {n} Dirichlet(\\alpha _{s})}{max (Dirichlet (\\alpha_{s'})  \\text{for} s' \\in d)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augmented_embed_sent(text, alpha = 0.5):\n",
    "    out = np.array([0.]*128)\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    for text in nltk.sent_tokenize(text):\n",
    "        out += embed(text, lda, dictionary)\n",
    "    \n",
    "    out = alpha + alpha * out/max(out)\n",
    "    \n",
    "    return out/sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00774795,  0.00774795,  0.01549591,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00826203,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795,  0.00774795,  0.00774795,\n",
       "        0.00774795,  0.00774795,  0.00774795])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_embed_sent(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. augmented embed business tfidf   \n",
    "Finally we scale the most characteristic topic for each business by using tfidf  \n",
    "\n",
    "$\\theta_{d}(\\beta, b) = \\text{augmented}(d) * \\text{augmented}(d_{b}) * log \\frac{N_{s_{b}}}{|s \\in d_{b} : t \\in s| + \\gamma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/b_tfidf.pickle', 'rb') as f:\n",
    "    business_tfidf_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augmented_tf_business_tfidf(text, business_id, alpha = 0.5, minimum_probability = 0.0):\n",
    "    tf = augmented_embed_sent(text, alpha)\n",
    "    btfidf = business_tfidf_dict[business_id]\n",
    "    out = np.multiply(tf, btfidf)\n",
    "    if sum(out) == 0.0:\n",
    "        print ('Business has too low tfidf')\n",
    "        return np.array([0.]*128)\n",
    "    return out/sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00823063,  0.00850354,  0.01664549,  0.00719256,  0.00975305,\n",
       "        0.00760763,  0.00659569,  0.00822557,  0.00680431,  0.00859019,\n",
       "        0.00837075,  0.00803532,  0.00643756,  0.0072259 ,  0.0071042 ,\n",
       "        0.00712047,  0.00890572,  0.00725679,  0.00763082,  0.00725775,\n",
       "        0.00715523,  0.00645387,  0.00853101,  0.00701878,  0.00760285,\n",
       "        0.00852419,  0.00732912,  0.00807017,  0.00682027,  0.00682255,\n",
       "        0.00848553,  0.00794957,  0.00673923,  0.00886794,  0.0069194 ,\n",
       "        0.0091204 ,  0.00691605,  0.00751699,  0.00752805,  0.00734154,\n",
       "        0.00678523,  0.00810036,  0.00721915,  0.00835811,  0.00693084,\n",
       "        0.00806947,  0.00782072,  0.00602946,  0.00724106,  0.0074844 ,\n",
       "        0.00704896,  0.00832074,  0.00604295,  0.01052008,  0.00698405,\n",
       "        0.00875748,  0.00790735,  0.00800847,  0.00767964,  0.00811386,\n",
       "        0.00813932,  0.0071059 ,  0.00662446,  0.00913259,  0.00764019,\n",
       "        0.00773215,  0.00784225,  0.00719325,  0.00728189,  0.00650995,\n",
       "        0.00707205,  0.00847746,  0.00767394,  0.00755538,  0.00780772,\n",
       "        0.00632574,  0.00877013,  0.00679695,  0.00817601,  0.00730591,\n",
       "        0.00673781,  0.00870355,  0.00957144,  0.00783581,  0.00723502,\n",
       "        0.00744935,  0.00897275,  0.00749353,  0.0073579 ,  0.00835172,\n",
       "        0.00971543,  0.00839499,  0.00807348,  0.00836073,  0.00809663,\n",
       "        0.00800996,  0.00778569,  0.007417  ,  0.00806494,  0.00887758,\n",
       "        0.00656923,  0.00727602,  0.00885843,  0.00995152,  0.00806313,\n",
       "        0.00819508,  0.00796144,  0.00694623,  0.00744231,  0.00722994,\n",
       "        0.00819394,  0.00981393,  0.00762699,  0.0090402 ,  0.00630221,\n",
       "        0.00765426,  0.00839902,  0.00699101,  0.00653431,  0.00774688,\n",
       "        0.00847475,  0.00659439,  0.00719306,  0.00768594,  0.00689393,\n",
       "        0.00759482,  0.00836494,  0.00810451])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_tf_business_tfidf(sample, 'jQsNFOzDpxPmOurSWCg1vQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a wrapper class that implements all the embeddings more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedder = TextEmbedder(model = lda, dictionary = dictionary, business_tfidf = business_tfidf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to analyze how much LDA can extract specific topics, rather than general topics. For this reason, we first filtered the business with 'chinese' tag, and get 3773 businesses. We also filtered corresponding reviews and also filtered meaningless ones (we found some reviews only ontained 1 word because of typo) and ended up using 175281 reviews. We also used 2 business that contains most reviews, pH0BLkL4cbxKzu471VZnuA and X8c23dur0ll2D9XTu-I8Qg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use top2 growing, declining business for case study\n",
    "case1 = reviews[reviews['business_id'] == 'pH0BLkL4cbxKzu471VZnuA'] # growing \n",
    "\n",
    "sample = random.sample(set(reviews['review_id'].values), len(case1))\n",
    "sample1 = reviews[reviews['review_id'].isin(sample)]\n",
    "\n",
    "case2 = reviews[reviews['business_id'] == 'X8c23dur0ll2D9XTu-I8Qg'] # declining\n",
    "\n",
    "sample = random.sample(set(reviews['review_id'].values), len(case2))\n",
    "sample2 = reviews[reviews['review_id'].isin(sample)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also preprocessed the labels so that it reflects business specific performance. For this reasons, we decided to take the difference between each review rating and the rating of business and defined as 'offsets'. Similarly, we grouped the reviews based on quarter (every 3 month), and defined the offsets. These valuses mean how positive/negative each review or quarter is to the specific business. Finally, in order to build a classifier, we label them into 3 classes (positives, neutrals, negatives) and 2 classes (positives and negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label data, try to predict simple labels -- positive(1), negative(-1) or average(0)\n",
    "def labels(offsets):\n",
    "    if offsets < 0.0:\n",
    "        return -1\n",
    "    else:\n",
    "        return int(offsets > 0.0)\n",
    "labels = np.vectorize(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>business_offset</th>\n",
       "      <th>quarter_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48835</th>\n",
       "      <td>xehs7BV3CG_prSOVguvWRA</td>\n",
       "      <td>pH0BLkL4cbxKzu471VZnuA</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48836</th>\n",
       "      <td>xDjjmA611W3wiYHBgiDd6g</td>\n",
       "      <td>pH0BLkL4cbxKzu471VZnuA</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>de4DtH_xNjYlbfqUY0Kj2A</td>\n",
       "      <td>pH0BLkL4cbxKzu471VZnuA</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.059701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>akys4VJyn7Ve4qhMK1zpgA</td>\n",
       "      <td>pH0BLkL4cbxKzu471VZnuA</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>t8KdC_-TNI0xUwLsSUKrbw</td>\n",
       "      <td>pH0BLkL4cbxKzu471VZnuA</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.940299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id             business_id  stars  business_offset  \\\n",
       "48835  xehs7BV3CG_prSOVguvWRA  pH0BLkL4cbxKzu471VZnuA      5              1.0   \n",
       "48836  xDjjmA611W3wiYHBgiDd6g  pH0BLkL4cbxKzu471VZnuA      2             -2.0   \n",
       "48837  de4DtH_xNjYlbfqUY0Kj2A  pH0BLkL4cbxKzu471VZnuA      5              1.0   \n",
       "48838  akys4VJyn7Ve4qhMK1zpgA  pH0BLkL4cbxKzu471VZnuA      4              0.0   \n",
       "48839  t8KdC_-TNI0xUwLsSUKrbw  pH0BLkL4cbxKzu471VZnuA      1             -3.0   \n",
       "\n",
       "       quarter_offset  \n",
       "48835        1.125000  \n",
       "48836       -2.022472  \n",
       "48837        1.059701  \n",
       "48838       -0.296296  \n",
       "48839       -2.940299  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1[['review_id', 'business_id', 'stars', 'business_offset', 'quarter_offset']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used classification task as a metric of how representitive each embedding is. In order to measure the accuracy, we used K-fold stratified cross validation and collect training accuracy and test accuracy for 3 different classifiers (SVM, xgboost, fully connected neural network). We also experiment with binary classifications (either positive and negative) and in this case, we collected precision, recall, and f-1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cmat_to_accuracy(mat):\n",
    "    size = len(mat)\n",
    "    total = sum(sum(mat))\n",
    "    correct = 0\n",
    "    for i in range(size):\n",
    "        correct += mat[i,i]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiments(embed, yb):\n",
    "    avg = []\n",
    "    train = []\n",
    "\n",
    "    avg_svm = []\n",
    "    train_svm = []\n",
    "    for x, i in skf.split(embed, yb):\n",
    "        train_x = embed[x]\n",
    "        train_y = yb[x]\n",
    "        test_x = embed[i]\n",
    "        test_y = yb[i]\n",
    "        model = XGBClassifier()\n",
    "        model.fit(train_x, train_y)\n",
    "        train.append(cmat_to_accuracy(confusion_matrix(model.predict(train_x), train_y)))\n",
    "        avg.append(cmat_to_accuracy(confusion_matrix(model.predict(test_x), test_y)))\n",
    "        svm = SVC()\n",
    "        svm.fit(train_x, train_y)\n",
    "        train_svm.append(cmat_to_accuracy(confusion_matrix(svm.predict(train_x), train_y)))\n",
    "        avg_svm.append(cmat_to_accuracy(confusion_matrix(svm.predict(test_x), test_y)))\n",
    "\n",
    "    print ('svm')\n",
    "    print ('Train set\\n avg: {}'.format(np.mean(train_svm)), 'var: {}'.format(np.var(train_svm)))\n",
    "    print ('Test set\\n avg: {}'.format(np.mean(avg_svm)), 'var: {}'.format(np.var(avg_svm)))\n",
    "\n",
    "    print ('xgboost')\n",
    "    print ('Train set\\n avg: {}'.format(np.mean(train)), 'var: {}'.format(np.var(train)))\n",
    "    print ('Test set\\n avg: {}'.format(np.mean(avg)), 'var: {}'.format(np.var(avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict business offset (3 labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = case1['business_offset'].values\n",
    "y = labels(y)\n",
    "\n",
    "y2 = sample1['business_offset'].values\n",
    "y2 = labels(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run classifiers with random businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.4907597676149698 var: 1.6195278891527505e-08\n",
      "Test set\n",
      " avg: 0.49075997903524715 var: 2.5857951078284415e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.7615508054434093 var: 3.469828938301724e-06\n",
      "Test set\n",
      " avg: 0.6160137804982282 var: 0.0001558430957570951\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.embed_sent(t) for t, b in zip(sample1['text'].values, sample1['business_id'].values)])\n",
    "run_experiments(embed, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.4907597676149698 var: 1.6195278891527505e-08\n",
      "Test set\n",
      " avg: 0.49075997903524715 var: 2.5857951078284415e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.7846512627621912 var: 3.779457095430929e-05\n",
      "Test set\n",
      " avg: 0.6232051526493962 var: 0.00021488974937629448\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_embed_text(t) for t, b in zip(sample1['text'].values, sample1['business_id'].values)])\n",
    "run_experiments(embed, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.4907597676149698 var: 1.6195278891527505e-08\n",
      "Test set\n",
      " avg: 0.49075997903524715 var: 2.5857951078284415e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.8633237913377462 var: 7.954979717625874e-05\n",
      "Test set\n",
      " avg: 0.5878110866050299 var: 0.001295346212860533\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_tf_business_tfidf(t, b) for t, b in zip(sample1['text'].values, sample1['business_id'].values)])\n",
    "run_experiments(embed, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run classifiers with specific businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.4517454988315065 var: 5.3695907774017384e-08\n",
      "Test set\n",
      " avg: 0.4517472905101771 var: 8.657901575484515e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.7737423060465421 var: 0.00012103679548114681\n",
      "Test set\n",
      " avg: 0.5657229711868886 var: 0.00040361104595184986\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.embed_sent(t) for t, b in zip(case1['text'].values, case1['business_id'].values)])\n",
    "run_experiments(embed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.4517454988315065 var: 5.3695907774017384e-08\n",
      "Test set\n",
      " avg: 0.4517472905101771 var: 8.657901575484515e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.7931203712846845 var: 4.605165625937355e-05\n",
      "Test set\n",
      " avg: 0.5590510177108114 var: 0.00026273726618568285\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_embed_text(t) for t, b in zip(case1['text'].values, case1['business_id'].values)])\n",
    "run_experiments(embed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.4517454988315065 var: 5.3695907774017384e-08\n",
      "Test set\n",
      " avg: 0.4517472905101771 var: 8.657901575484515e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.790040650406504 var: 2.0497314965717702e-05\n",
      "Test set\n",
      " avg: 0.5405762622257468 var: 0.0003982131618355532\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_tf_business_tfidf(t, b) for t,b in zip(case1['text'].values, case1['business_id'].values)])\n",
    "run_experiments(embed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Predict business offset (2 labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run classifiers with random businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1b = case1[case1['business_offset'] != 0.0]\n",
    "\n",
    "reviewsb = reviews[reviews['business_offset'] != 0.0]\n",
    "sample = random.sample(set(reviewsb['review_id'].values), len(case1))\n",
    "sample1b = reviewsb[reviewsb['review_id'].isin(sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb = case1b['business_offset'].values\n",
    "yb = labels(yb)\n",
    "\n",
    "y2b = sample1b['business_offset'].values\n",
    "y2b = labels(y2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.5734086271257548 var: 1.5483592887082267e-08\n",
      "Test set\n",
      " avg: 0.5734086688814859 var: 2.4683642457263684e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.834447436576653 var: 3.5282840324723616e-05\n",
      "Test set\n",
      " avg: 0.7289598087345608 var: 0.00011339071013990502\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.embed_sent(t) for t, b in zip(sample1b['text'].values, sample1b['business_id'].values)])\n",
    "run_experiments(embed, y2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.5734086271257548 var: 1.5483592887082267e-08\n",
      "Test set\n",
      " avg: 0.5734086688814859 var: 2.4683642457263684e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.8560062210533875 var: 1.3571059885540267e-05\n",
      "Test set\n",
      " avg: 0.7192029887416445 var: 4.3955312735830466e-05\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_embed_text(t) for t, b in zip(sample1b['text'].values, sample1b['business_id'].values)])\n",
    "run_experiments(embed, y2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.5734086271257548 var: 1.5483592887082267e-08\n",
      "Test set\n",
      " avg: 0.5734086688814859 var: 2.4683642457263684e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.8948921200946751 var: 9.954328574180222e-06\n",
      "Test set\n",
      " avg: 0.6992107839344179 var: 0.00029403906315437003\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_tf_business_tfidf(t, b) for t, b in zip(sample1b['text'].values, sample1b['business_id'].values)])\n",
    "run_experiments(embed, y2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run classifiers with specific businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.6312769821752896 var: 5.1213820734668746e-08\n",
      "Test set\n",
      " avg: 0.6312782032438565 var: 8.238484328303165e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.9011839207933523 var: 1.0371038432887464e-06\n",
      "Test set\n",
      " avg: 0.7847889430391171 var: 5.775199855071701e-05\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.embed_sent(t) for t, b in zip(case1b['text'].values, case1b['business_id'].values)])\n",
    "run_experiments(embed, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.6312769821752896 var: 5.1213820734668746e-08\n",
      "Test set\n",
      " avg: 0.6312782032438565 var: 8.238484328303165e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.9133789800215375 var: 1.720690797712318e-05\n",
      "Test set\n",
      " avg: 0.7934091436528197 var: 8.178626590605422e-05\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_embed_text(t) for t, b in zip(case1b['text'].values, case1b['business_id'].values)])\n",
    "run_experiments(embed, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.6312769821752896 var: 5.1213820734668746e-08\n",
      "Test set\n",
      " avg: 0.6312782032438565 var: 8.238484328303165e-07\n",
      "xgboost\n",
      "Train set\n",
      " avg: 0.9166049472009258 var: 6.49514206750849e-05\n",
      "Test set\n",
      " avg: 0.7890745468141616 var: 0.000572446360040989\n"
     ]
    }
   ],
   "source": [
    "embed = np.array([embedder.augmented_tf_business_tfidf(t, b) for t,\\\n",
    "                  b in zip(case1b['text'].values, case1b['business_id'].values)])\n",
    "run_experiments(embed, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predict quarter offset (3 labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_by_enum(data, embedder, enum = 0, binary = False):\n",
    "    # select regions\n",
    "    if binary:\n",
    "        data = data[data['quarter_offset'] != 0]\n",
    "    label = data['quarter_offset']\n",
    "    # create labels\n",
    "    if enum == 2: \n",
    "        embed = np.array([embedder.augmented_embed_text(t) for t in data['text'].values])\n",
    "    elif enum == 3: \n",
    "        embed = np.array([embedder.user_tfidf_embed(t, u) for t, u in zip(data['text'].values, data['user_id'].values)])\n",
    "    elif enum == 4: \n",
    "        embed = np.array([embedder.user_tf_business_idf(t, b) for t, b in zip(data['text'].values, data['business_id'].values)])\n",
    "    elif enum == 5: \n",
    "        embed = np.array([embedder.user_tfidf_business_idf(t, u, b) for t, u, b in zip(data['text'].values, data['user_id'].values, data['business_id'].values)])\n",
    "    \n",
    "    elif enum == 0: \n",
    "        embed = np.array([embedder.embed(t) for t in data['text'].values])\n",
    "    elif enum == 1:\n",
    "        embed = np.array([embedder.embed_sent(t) for t in data['text'].values])\n",
    "    \n",
    "    elif enum == 6:\n",
    "        embed = np.array([embedder.augmented_tf_business_tfidf(t, b) for t, b in zip(data['text'].values, data['business_id'].values)])\n",
    "    else:\n",
    "        print ('enum {} is not supported'.format(enum))\n",
    "        return None\n",
    "    return embed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper to build sequential data, fitting a linear regression and taking a slope\n",
    "# on a side note, I also tried with SVR but the line of best fit tends to be horizontal, aka simply finding the average\n",
    "def build_data(df, enum):\n",
    "    qs = sorted(list(set(df['quarter'])))\n",
    "    X = []\n",
    "    y = []\n",
    "    for q in qs:\n",
    "        filtered = df[df['quarter'] == q]\n",
    "        embed, labels = embed_by_enum(filtered, embedder, enum)\n",
    "        \n",
    "        # fit on the sequence, store the slope\n",
    "        regr = LinearRegression()\n",
    "        regr.fit(np.arange(len(embed)).reshape(-1,1),embed)\n",
    "        X.append(regr.coef_.reshape(1,-1)[0])\n",
    "        \n",
    "        y.append(list(set(filtered['quarter_avg'].values))[0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = build_data(case1, 1)\n",
    "y = y - case1['business_stars'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.5409523809523809 var: 0.00023219954648526052\n",
      "Test set\n",
      " avg: 0.5460317460317461 var: 0.0025799949609473416\n",
      "xgboost\n",
      "Train set\n",
      " avg: 1.0 var: 0.0\n",
      "Test set\n",
      " avg: 0.8063492063492064 var: 0.013948097757621566\n"
     ]
    }
   ],
   "source": [
    "run_experiments(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2,y2 = build_data(case1, 2)\n",
    "y2 = y2 - case1['business_stars'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kentatakatsu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "Train set\n",
      " avg: 0.5409523809523809 var: 0.00023219954648526052\n",
      "Test set\n",
      " avg: 0.5460317460317461 var: 0.0025799949609473416\n",
      "xgboost\n",
      "Train set\n",
      " avg: 1.0 var: 0.0\n",
      "Test set\n",
      " avg: 0.4222222222222222 var: 0.04460569412950366\n"
     ]
    }
   ],
   "source": [
    "y2 = labels(y2)\n",
    "run_experiments(x2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x3,y3 = build_data(case1, 6)\n",
    "y3 = y3 - case1['business_stars'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = labels(y3)\n",
    "run_experiments(x3,y3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
