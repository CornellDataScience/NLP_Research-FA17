{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from utils import * \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_df(datapass):\n",
    "    '''\n",
    "    Load the json file and parse the file to pandas dataframe format\n",
    "    \n",
    "    Input:\n",
    "        datapass(str) : directory to the json file\n",
    "    Output:\n",
    "        df(dataframe) : pandas dataframe object\n",
    "    '''\n",
    "    \n",
    "    data = [] \n",
    "    with open(datapass, 'r', encoding='utf-8') as data_file: \n",
    "        for f in data_file:\n",
    "            data.append(json.loads(f))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "#business and review json files --> dataframes\n",
    "\n",
    "DATAPASS1 = 'C:\\\\Users\\\\cdchang\\\\Downloads\\\\yelp_dataset\\\\dataset\\\\business.json'\n",
    "DATAPASS2 = 'C:\\\\Users\\\\cdchang\\\\Downloads\\\\yelp_dataset\\\\dataset\\\\review.json'\n",
    "\n",
    "#business = json_df(DATAPASS1)\n",
    "reviews = json_df(DATAPASS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, n_top_words):\n",
    "    '''\n",
    "    display topic with n_top_words in a decsending order of weight\n",
    "    \n",
    "    Input:\n",
    "        model : directory for vectorizer\n",
    "        feature_names : list of vocabulary \n",
    "        n_top_words(int) : number of words to display for each topic\n",
    "\n",
    "    '''\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % topic_index)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words:-1]]))\n",
    "\n",
    "def load_topic_model(vectorizer_file_name, topic_model_file_name):\n",
    "    with open(vectorizer_file_name, \"rb\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    with open(topic_model_file_name, \"rb\") as f:\n",
    "        topic_model = pickle.load(f)\n",
    "    return vectorizer, topic_model\n",
    "\n",
    "#pizza\n",
    "\n",
    "path1 = 'C:\\\\Users\\\\cdchang\\\\Desktop\\\\yelp-topic-model\\\\model\\\\pizza_tfidf_vectorizer.pkl'\n",
    "path2 = 'C:\\\\Users\\\\cdchang\\\\Desktop\\\\yelp-topic-model\\\\model\\\\pizza_nmf.pkl'\n",
    "path3 = 'C:\\\\Users\\\\cdchang\\\\Desktop\\\\yelp-topic-model\\\\model\\\\pizza_count_vectorizer.pkl'\n",
    "path4 = 'C:\\\\Users\\\\cdchang\\\\Desktop\\\\yelp-topic-model\\\\model\\\\pizza_lda.pkl'\n",
    "vec, tm = load_topic_model(path1, path2)\n",
    "vec2, tm2 = load_topic_model(path3, path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.35318219e-02,   0.00000000e+00,   4.09802948e-02, ...,\n",
       "          7.96897006e-02,   3.98538679e-02,   0.00000000e+00],\n",
       "       [  7.86209423e-02,   1.88202333e-01,   3.27705625e-02, ...,\n",
       "          2.97546372e-02,   9.73259392e-02,   0.00000000e+00],\n",
       "       [  3.54345514e-01,   2.62719244e+00,   1.12437870e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  4.13455229e-02,   0.00000000e+00,   4.27813136e-04, ...,\n",
       "          0.00000000e+00,   6.59642074e-02,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          3.22001617e-02,   1.51870707e-01,   0.00000000e+00],\n",
       "       [  1.37329447e-02,   0.00000000e+00,   7.26685010e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k index embedding methods\n",
    "\n",
    "def get_k_index(sentence, vec, tm):\n",
    "    '''\n",
    "    Given a sentence (not the entire review), generate a distribution of topic \n",
    "    by adding the weight of topic per words.\n",
    "    I initialy tried with the product of topic weights but ended up getting 0s.\n",
    "    \n",
    "    Also produce the probability vector by scaling the vector so that the sum will be 1.\n",
    "    \n",
    "    Input:\n",
    "        sentence(str) : a sentence to be studied\n",
    "        vec : vectorizer object \n",
    "        tm : topic model object \n",
    "    Output:\n",
    "        k_index (list) : the vector of length k (k = number of topic). \n",
    "        prob (list) : k_index but scaled so that the sum of inputs becomes 1 \n",
    "    '''\n",
    "    analyzer = vec.build_analyzer()\n",
    "    topic_words = vec.get_feature_names()\n",
    "    tm_mat = tm.components_\n",
    "    k_index = np.array([0.]*tm_mat.shape[0])\n",
    "    for word in analyzer(sentence):\n",
    "        if word in topic_words:\n",
    "            k_index += tm_mat[:, topic_words.index(word)]\n",
    "    if sum(k_index) == 0:\n",
    "        prob = np.array([0.]*tm_mat.shape[0])\n",
    "    else:\n",
    "        prob = k_index/(sum(k_index))\n",
    "    return k_index, prob\n",
    "\n",
    "\n",
    "def get_avg_k_index_for_doc(doc, vec, tm):\n",
    "    sent_text = nltk.sent_tokenize(doc)\n",
    "    if (int(len(sent_text)) == 0):\n",
    "        return []\n",
    "    else:\n",
    "        tm_mat = tm.components_\n",
    "        k_index = np.array([0.]*tm_mat.shape[0])\n",
    "        for s in sent_text:\n",
    "            k, p = get_k_index(s, vec, tm)\n",
    "            k_index += p\n",
    "        return k_index/int(len(sent_text))\n",
    "\n",
    "#use this \n",
    "def business_id_retrieval(cat, business):\n",
    "    '''\n",
    "    Input:\n",
    "        cat(str) : category\n",
    "        business(dataframe) : the business data\n",
    "    Output:\n",
    "        id_list(set) : business ids of a particular category\n",
    "    '''\n",
    "    id_list = set()\n",
    "    id_list = []\n",
    "    idx = 0\n",
    "    for row in business.values:\n",
    "        categories = row[3]\n",
    "        if cat in categories:\n",
    "            id_list.append(row[2])\n",
    "            \n",
    "    return id_list\n",
    "\n",
    "#business_id_retrieval('Pizza', business)\n",
    "\n",
    "#get_avg_k_index_for_doc(\"uYHaNptLzDLoV_JZ_MuzUA\",vec, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-6ca3705de860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweight_vectors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mtopic_points\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_points\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'k-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "#ultimately: find subset of pizza / chinese restaurants \n",
    "#set-up methods\n",
    "\n",
    "#for a single business_id\n",
    "def examine_reviews(b_id, reviews):\n",
    "    '''\n",
    "    concatenate a particular business's review text with the corresponding dates for time series analysis\n",
    "    \n",
    "    Input: specific business id, review json file\n",
    "    Output: concatenated review text & date columns for a particular business id\n",
    "    '''\n",
    "    subset = reviews.loc[reviews['business_id'] == b_id]\n",
    "    date = subset.loc[:, 'date']\n",
    "    text = subset.loc[:, 'text']\n",
    "    return pd.concat([date, text], axis=1, join='inner')\n",
    "\n",
    "#trying with single business id / not tested\n",
    "def k_index_emb_single(b_id, reviews):\n",
    "    \n",
    "    '''\n",
    "    input: business id, reviews json\n",
    "    output: weight vectors for each topic, for every review from a particular business, concatenated (sorted by time)\n",
    "    '''\n",
    "    \n",
    "    subset = examine_reviews(b_id, reviews)\n",
    "    s = subset.sort_values(by=\"date\") #sorting by timestamp\n",
    "    text_only = s.loc[:, 'text']\n",
    "    count= 0\n",
    "    weight_vectors = []\n",
    "    for t in text_only:\n",
    "        if (count==0):\n",
    "            weight_vectors.append(get_avg_k_index_for_doc(t,vec,tm))\n",
    "        else:\n",
    "            newcol = get_avg_k_index_for_doc(t,vec,tm)\n",
    "            weight_vectors = np.vstack([weight_vectors, newcol])\n",
    "        count = count+1    \n",
    "    return weight_vectors\n",
    "\n",
    "def examine_reviews2(cat, business, reviews):\n",
    "    '''\n",
    "    concatenate a category's review text with the corresponding dates for time series analysis\n",
    "    Input: specific category, review json file\n",
    "    Output: concatenated review text & date columns for a particular category (not business specific) \n",
    "    '''\n",
    "    id_list = business_id_retrieval(cat, business)\n",
    "    return reviews.loc[reviews['business_id'].isin(id_list)]\n",
    "\n",
    "#sort by timestamp\n",
    "subset = examine_reviews(\"HRFJlSAP_EBU_MpPPmpUDQ\", reviews)\n",
    "s = subset.sort_values(by=\"date\") #sorting by timestamp\n",
    "\n",
    "weight_vectors = k_index_emb_single('HRFJlSAP_EBU_MpPPmpUDQ', reviews)\n",
    "topic_points = list()\n",
    "\n",
    "for wv in weight_vectors:\n",
    "    topic_points.append(weight_vectors[wv,0])\n",
    "plt.plot(topic_points,s.loc[:,'date'], 'k-', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find trend\n",
    "import matplotlib as plt\n",
    "def plot_per_topic(b_id, reviews, topic_num):\n",
    "    weight_vectors = k_index_emb_single(b_id, reviews)\n",
    "    text_only = s.loc[:, 'text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
