{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Categorizing Yelp Businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a baseline based on the article from Yelp Software Team, [Automatically Categorizing Yelp Businesses](https://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html)  \n",
    "Not using text information, guess the multi-label assignment of business. Here, all sample set contains 'Chinese' tag and try to see if the model can detect more subtle labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import * \n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from co_occurrence_net.category_map import CategoryMap\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "chinese_business = pd.read_csv('chinese_business.csv', index_col = False)\n",
    "chinese_reviews = pd.read_csv('chinese_review_clean.csv', index_col = False)\n",
    "chinese_business =  [eval(i) for i in chinese_business['categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = CategoryMap()\n",
    "G.build_graph(chinese_business['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asian Fusion', 449),\n",
       " ('Food', 398),\n",
       " ('Fast Food', 260),\n",
       " ('Thai', 237),\n",
       " ('Dim Sum', 230),\n",
       " ('Buffets', 211),\n",
       " ('Japanese', 179),\n",
       " ('Seafood', 163),\n",
       " ('Sushi Bars', 158),\n",
       " ('Specialty Food', 134)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.get_subcategories('Chinese')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Series of Binary Classifiers: One for Each Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a business of a given category \n",
    "\n",
    "We extract terms from names and reviews, using standard lexical analysis techniques of tokenization, normalization (e.g. lowercasing), and stop word filtering. If the business has been categorized as part of a chain (which we’ll describe in an upcoming blog post!) we’ll include that chain’s URL as a feature, and if the business has NAICS codes from one of our data partners, we’ll include those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(business_df, review_df, topic):\n",
    "    '''\n",
    "    Split the original data into 2 classes, ones that includes topic label and don't\n",
    "    '''\n",
    "    print ('topic: {}'.format(topic))\n",
    "    includes = set()\n",
    "    not_includes = set()\n",
    "    for i, topics in enumerate(business_df['categories']):\n",
    "        if topic in topics:\n",
    "            includes.add(business_df.iloc[i]['business_id'])\n",
    "        else:\n",
    "            not_includes.add(business_df.iloc[i]['business_id'])\n",
    "    review_included = review_df.loc[review_df['business_id'].isin(includes)]\n",
    "    review_not_included = review_df.loc[review_df['business_id'].isin(not_includes)]\n",
    "    print ('include topic:     {} business, {} reviews'.format(len(includes), len(review_included)))\n",
    "    print ('not include topic: {} business, {} reviews'.format(len(not_includes), len(review_not_included)))\n",
    "    \n",
    "    return review_included, review_not_included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: Dim Sum\n",
      "include topic:     230 business, 21960 reviews\n",
      "not include topic: 3545 business, 156189 reviews\n"
     ]
    }
   ],
   "source": [
    "t, f = data_split(chinese_business, chinese_reviews, G.get_subcategories('Chinese')[4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp used following features:\n",
    "- Tokenized Name\n",
    "- Tokenized Review\n",
    "- NAICS(we do not have an access)\n",
    "- country (we disregard)\n",
    "- Last Term in Name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genereate_feature(review_counter, name_counter, review_df, business_df, business_id):\n",
    "    '''\n",
    "    '''    \n",
    "    # filter the restaurant name\n",
    "    name = business_df.loc[business_df['business_id'] == business_id]['name']\n",
    "\n",
    "    # filter the reviews for the specified business\n",
    "    review = review_df.loc[review_df['business_id'] == business_id]['text']\n",
    "    # extract the last word of the restaurant\n",
    "    last_name = pd.Series(name_counter.build_analyzer()(name.values[0])[-1])\n",
    "\n",
    "    # feature length \n",
    "    name_length = len(name_counter.get_feature_names()) \n",
    "    review_length = len(review_counter.get_feature_names())\n",
    "    \n",
    "    # NAME + LAST NAME + REVIEW\n",
    "    name_feature = np.zeros(name_length)\n",
    "    last_name_feature = np.zeros(name_length)\n",
    "    review_feature = np.zeros(review_length)\n",
    "\n",
    "    for r, d in zip(name_counter.transform(name).indices, name_counter.transform(name).data):\n",
    "        name_feature[r] = d\n",
    "\n",
    "    for r, d in zip(name_counter.transform(last_name).indices, name_counter.transform(last_name).data):\n",
    "        last_name_feature[r] = d\n",
    "    \n",
    "    for r, d in zip(review_counter.transform(review).indices, review_counter.transform(review).data):\n",
    "        review_feature[r] = d\n",
    "    \n",
    "    feature = np.array(np.concatenate((name_feature, last_name_feature, review_feature),axis = 0))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set(review_counter, name_counter, review_df, business_df, topic):\n",
    "    '''\n",
    "    '''\n",
    "    # select business with topic tag\n",
    "    t, f = data_split(business_df, review_df, topic)\n",
    "    t_in =  (set(t['business_id']))\n",
    "    t_not_in =  (set(f['business_id']))\n",
    "    \n",
    "    # output dimension\n",
    "    name_length = len(name_counter.get_feature_names()) \n",
    "    review_length = len(review_counter.get_feature_names())\n",
    "    feature_length = 2*name_length + review_length\n",
    "    \n",
    "    X_in = np.array([])\n",
    "    Y_in = np.array([])\n",
    "    \n",
    "    for b in t_in:\n",
    "        feature = genereate_feature(review_counter, name_counter, review_df, business_df, b)\n",
    "        X_in = np.append(X_in, feature)\n",
    "        Y_in = np.append(Y_in, 1)\n",
    "    print ('positive set done')\n",
    "    \n",
    "    X_out = np.array([])\n",
    "    Y_out = np.array([])\n",
    "    \n",
    "    for b in t_not_in:\n",
    "        feature = genereate_feature(review_counter, name_counter, review_df, business_df, b)\n",
    "        X_out = np.append(X_out, feature)\n",
    "        Y_out = np.append(Y_out, 0)\n",
    "    print ('negative set done')\n",
    "    \n",
    "    \n",
    "    return X_in.reshape(len(t_in), feature_length), Y_in.reshape(-1,1), X_out.reshape(len(t_not_in), feature_length), Y_out.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genereate_feature(review_counter, name_counter, chinese_reviews, chinese_business, 'OygJyqypKFZJIZ6r9dML7w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: Dim Sum\n",
      "include topic:     230 business, 21960 reviews\n",
      "not include topic: 3545 business, 156189 reviews\n",
      "positive set done\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-8812911bdd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchinese_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchinese_business\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Dim Sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-330-c6517b5fb1ef>\u001b[0m in \u001b[0;36mcreate_data_set\u001b[0;34m(review_counter, name_counter, review_df, business_df, topic)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_not_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenereate_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusiness_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mX_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mY_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-316-cbffe1c52b1f>\u001b[0m in \u001b[0;36mgenereate_feature\u001b[0;34m(review_counter, name_counter, review_df, business_df, business_id)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# extract the last word of the restaurant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlast_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# feature length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "create_data_set(review_counter, name_counter, chinese_reviews, chinese_business,'Dim Sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.append(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(12).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
