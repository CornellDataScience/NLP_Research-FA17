{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Topic Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import * \n",
    "from ldaembedder.lda_model import LDAembedder\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual reviews contain too much variance, and cannot generalize the topic distribution enough. We are going to explore more robust high dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "chinese_business = pd.read_csv('chinese_business.csv', index_col = False)\n",
    "chinese_reviews = pd.read_csv('chinese_review.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained topic models\n",
    "with open(\"model/chinese_counter.pkl\", \"rb\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "with open(\"model/chinese.pkl\", \"rb\") as f:\n",
    "        topic_model = pickle.load(f)\n",
    "lda = LDAembedder(model = topic_model, counter = vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = lda.embed(chinese_reviews['text'].values[5], method = 'additive')\n",
    "y = lda.embed(chinese_reviews['text'].values[5], method = 'multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Decent customer service but the food was awful. It was cold and had no sauce at all. I was expecting it to be good but this place really went down hill. I will never eat here again.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_reviews['text'].values[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    '''\n",
    "    Simple implementation of auto-encoder\n",
    "    '''\n",
    "    def __init__(self, k, hidden):\n",
    "        self.n_input = k\n",
    "        self.n_hidden = hidden\n",
    "        self.weights = {\n",
    "            'encoder_h': tf.Variable(tf.random_normal([self.n_input, self.n_hidden], dtype=tf.float32)),\n",
    "            'decoder_h': tf.Variable(tf.random_normal([self.n_hidden, self.n_input], dtype=tf.float32)),\n",
    "        }\n",
    "        self.biases = {\n",
    "            'encoder_b': tf.Variable(tf.random_normal([self.n_hidden], dtype=tf.float32)),\n",
    "            'decoder_b': tf.Variable(tf.random_normal([self.n_input], dtype=tf.float32)),\n",
    "        }\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        feed = tf.add(tf.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
    "        layer = tf.nn.sigmoid(feed)\n",
    "        return layer\n",
    "\n",
    "    def decoder(self, x):\n",
    "        feed = tf.add(tf.matmul(x, self.weights['decoder_h']), self.biases['decoder_b'])\n",
    "        layer = tf.nn.sigmoid(feed)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 23.911403656\n",
      "true: [ 0.00624469  0.00624469  0.01304741  0.00964605  0.00624469  0.00964605\n",
      "  0.00624469  0.00964605  0.00624469  0.00624469  0.01304741  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.00624469  0.01304741  0.00624469\n",
      "  0.00624469  0.00624469  0.01304741  0.00624469  0.00624469  0.00964605\n",
      "  0.01644877  0.00624469  0.00624469  0.00624469  0.00624469  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.01161978  0.00624469  0.00624469\n",
      "  0.00624469  0.01644877  0.01644877  0.00624469  0.01304741  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.01304741  0.00624469  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.00624469  0.00624469  0.00964605\n",
      "  0.00964605  0.00624469  0.0082942   0.00964605  0.00964605  0.00964605\n",
      "  0.00624469  0.00964605  0.00624469  0.00624469  0.00624469  0.00964605\n",
      "  0.00624469  0.00964605  0.00624469  0.00964605  0.00624469  0.00624469\n",
      "  0.00759653  0.00624469  0.00624469  0.00964605  0.00624469  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.00624469  0.00624469  0.02325149\n",
      "  0.00964605  0.00624469  0.00624469  0.00624469  0.00624469  0.00624469\n",
      "  0.00624469  0.01985013  0.00624469  0.00624469  0.00624469  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.00624469  0.00624469  0.00964605\n",
      "  0.00624469  0.00624469  0.00624469  0.00624469  0.01985013  0.00624469\n",
      "  0.00624469  0.00624469  0.00624469  0.00624469  0.00964605  0.00624469\n",
      "  0.00624469  0.00624469  0.00964605  0.00624469  0.00767231  0.00624469\n",
      "  0.00624469  0.00624469  0.00964605  0.00624469  0.00624469  0.00964605\n",
      "  0.01304741  0.00624469]\n",
      "pred: [[ 0.06262939  0.02517719  0.00580721  0.06016576  0.01317318  0.0299097\n",
      "   0.0535433   0.01585462  0.01753201  0.04642751  0.00334018  0.04267607\n",
      "   0.03276825  0.076103    0.02164228  0.07353069  0.0274592   0.05938335\n",
      "   0.06306716  0.05283463  0.04814008  0.04916494  0.08986427  0.03893092\n",
      "   0.05655942  0.02350926  0.01898608  0.04692698  0.01863406  0.07252719\n",
      "   0.00318163  0.06324476  0.02438218  0.09048522  0.01456706  0.05465247\n",
      "   0.03966969  0.04867167  0.08363347  0.06095643  0.02296929  0.00377033\n",
      "   0.07995249  0.08522521  0.00542497  0.07940964  0.00515245  0.05718113\n",
      "   0.08762249  0.01702879  0.07241446  0.07233529  0.00434361  0.04031019\n",
      "   0.05362877  0.00267583  0.04472487  0.04217368  0.00814349  0.00638818\n",
      "   0.01379635  0.00696858  0.01646471  0.0332792   0.00367029  0.04998421\n",
      "   0.02045714  0.02548234  0.04230635  0.05148494  0.0280238   0.0491771\n",
      "   0.00630499  0.08061621  0.05507544  0.07527004  0.05360702  0.07345046\n",
      "   0.07399552  0.02826178  0.05420066  0.07113162  0.01118847  0.0463344\n",
      "   0.07868174  0.05328904  0.01129886  0.04603966  0.00901172  0.08492466\n",
      "   0.09782043  0.08458581  0.05819302  0.00575601  0.075053    0.05523063\n",
      "   0.01541854  0.06907767  0.02968504  0.0737421   0.09142341  0.06717114\n",
      "   0.08839532  0.07940635  0.05221254  0.01252004  0.0731524   0.01945527\n",
      "   0.00185143  0.08065462  0.09222606  0.07307953  0.08250202  0.07306104\n",
      "   0.09117786  0.03722652  0.04672721  0.06066641  0.06216075  0.06186704\n",
      "   0.04665917  0.08171961  0.0997221   0.06568822  0.0567326   0.03922799\n",
      "   0.02813487  0.02455115]]\n",
      "Epoch: 0011 cost= 0.364323467\n",
      "Epoch: 0021 cost= 0.267698884\n",
      "Epoch: 0031 cost= 0.494980663\n",
      "Epoch: 0041 cost= 0.904174626\n",
      "Epoch: 0051 cost= 0.376555711\n",
      "Epoch: 0061 cost= 0.272558868\n",
      "Epoch: 0071 cost= 0.332120866\n",
      "Epoch: 0081 cost= 0.275477380\n",
      "Epoch: 0091 cost= 0.405388892\n",
      "Epoch: 0101 cost= 0.217842430\n",
      "Epoch: 0111 cost= 0.199703097\n",
      "Epoch: 0121 cost= 0.221668094\n",
      "Epoch: 0131 cost= 0.284334660\n",
      "Epoch: 0141 cost= 0.187337190\n",
      "Epoch: 0151 cost= 0.249928638\n",
      "Epoch: 0161 cost= 0.150624529\n",
      "Epoch: 0171 cost= 0.168501705\n",
      "Epoch: 0181 cost= 0.225844339\n",
      "Epoch: 0191 cost= 0.161190420\n",
      "true: [ 0.00615731  0.00615731  0.00615731  0.0103946   0.00615731  0.0103946\n",
      "  0.00615731  0.0103946   0.00615731  0.0103946   0.01463189  0.0103946\n",
      "  0.00615731  0.00615731  0.00615731  0.00615731  0.0103946   0.0103946\n",
      "  0.00615731  0.00615731  0.0103946   0.0103946   0.01463189  0.00615731\n",
      "  0.01463189  0.00615731  0.00615731  0.00615731  0.00615731  0.01463189\n",
      "  0.00615731  0.00615731  0.00615731  0.0103946   0.00615731  0.00615731\n",
      "  0.00615731  0.0103946   0.0103946   0.00615731  0.00615731  0.0103946\n",
      "  0.00615731  0.0103946   0.00615731  0.01463189  0.00615731  0.00615731\n",
      "  0.01463189  0.0103946   0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.01463189  0.01463189  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.0103946   0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.0103946   0.00615731  0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.04005561  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.00615731  0.00615731  0.00615731  0.02480918\n",
      "  0.0103946   0.00615731  0.00615731  0.00615731  0.0103946   0.00615731\n",
      "  0.00615731  0.00615731  0.00615731  0.01292916  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731  0.0103946   0.00615731  0.00615731  0.00615731\n",
      "  0.00615731  0.00615731]\n",
      "pred: [[ 0.00460386  0.00457281  0.00536653  0.00406418  0.00576986  0.00816465\n",
      "   0.01184105  0.04275849  0.00585464  0.00701431  0.00614828  0.00579842\n",
      "   0.00665387  0.00439266  0.00578896  0.00679855  0.00574054  0.00470458\n",
      "   0.00887017  0.00600838  0.00469921  0.00342545  0.00843419  0.00262177\n",
      "   0.00736225  0.00450294  0.00613377  0.00592451  0.00563747  0.00449059\n",
      "   0.00726953  0.00833596  0.00760782  0.02754692  0.005398    0.00507835\n",
      "   0.00511895  0.0070663   0.00521763  0.00567002  0.00590934  0.0055439\n",
      "   0.00564027  0.00535773  0.00156865  0.03406032  0.00608279  0.00587888\n",
      "   0.00743081  0.0103265   0.00693818  0.00965732  0.00705038  0.00773419\n",
      "   0.00506218  0.00512717  0.005477    0.0126255   0.01646136  0.01174503\n",
      "   0.00631879  0.00456264  0.00545984  0.0056929   0.00578609  0.0060303\n",
      "   0.00722128  0.00522862  0.00518213  0.00222351  0.00554658  0.00533249\n",
      "   0.0052342   0.01118867  0.00741154  0.00621697  0.00527576  0.00597509\n",
      "   0.02986527  0.00509506  0.01108742  0.00553755  0.00271467  0.00499554\n",
      "   0.00545266  0.00514081  0.01366488  0.00427895  0.00732958  0.0052507\n",
      "   0.01101455  0.00335985  0.00670897  0.00521211  0.00306486  0.00425224\n",
      "   0.00553859  0.00553229  0.00519546  0.00741009  0.00720257  0.01276443\n",
      "   0.00900679  0.00717522  0.01868212  0.00656317  0.00670483  0.01413836\n",
      "   0.00548503  0.00611613  0.00575995  0.00905613  0.00521244  0.00694278\n",
      "   0.00902794  0.00529481  0.00617025  0.0109379   0.00749796  0.00411385\n",
      "   0.00608154  0.00351118  0.00618278  0.00367911  0.00316104  0.00489298\n",
      "   0.00594075  0.00836423]]\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(128, 10)\n",
    "\n",
    "encoder_op = model.encoder(model.X)\n",
    "decoder_op = model.decoder(encoder_op)\n",
    "\n",
    "y_pred = decoder_op\n",
    "y_true = model.X\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "training_epochs = 200\n",
    "learning_rate = 0.01\n",
    "display_step = 10\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(tf.multiply(y_true - y_pred, 100), 2))\n",
    "regularizer = tf.reduce_mean(tf.nn.l2_loss(model.weights['encoder_h']))\n",
    "# cost = tf.reduce_mean(cost + 0.01 * regularizer)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_batch = int(len(chinese_reviews)/batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        for i in range(100):\n",
    "            indecies = random.sample(range(len(chinese_reviews_clean)), batch_size)\n",
    "            batch_xs = [lda.embed(i) for i in chinese_reviews_clean.iloc[indecies]['text']]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={model.X: batch_xs})\n",
    "        if epoch % display_step == 0:\n",
    "            y = sess.run(y_pred, feed_dict={model.X: batch_xs[0].reshape(1,-1)})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c))\n",
    "        if epoch == 0:   \n",
    "            print (\"true: {}\".format(batch_xs[0]))\n",
    "            print (\"pred: {}\".format(y))\n",
    "        if epoch == training_epochs-1:   \n",
    "            print (\"true: {}\".format(batch_xs[0]))\n",
    "            print (\"pred: {}\".format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be able to reconstruct some topics, but not all of them. This is an expected behavior since I deconstructed to 10% of original dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reviews are breaking my code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83656</th>\n",
       "      <td>mpG1R1ktTI3x1twNBsGvXQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-05</td>\n",
       "      <td>0</td>\n",
       "      <td>XTjQvPiXiLSkYXWGrH_taQ</td>\n",
       "      <td>5</td>\n",
       "      <td>v</td>\n",
       "      <td>0</td>\n",
       "      <td>CVhY1u5ZN4MidNWnRC-V6g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128621</th>\n",
       "      <td>6ZsiLiSWGK2pgw6t3LfzNQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>0</td>\n",
       "      <td>LyyrSgdpmDbJODdrH_GP5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>0kXhQEQHGVYVA-oxK8RrFQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   business_id  cool        date  funny  \\\n",
       "83656   mpG1R1ktTI3x1twNBsGvXQ     0  2015-07-05      0   \n",
       "128621  6ZsiLiSWGK2pgw6t3LfzNQ     0  2013-09-30      0   \n",
       "\n",
       "                     review_id  stars text  useful                 user_id  \n",
       "83656   XTjQvPiXiLSkYXWGrH_taQ      5    v       0  CVhY1u5ZN4MidNWnRC-V6g  \n",
       "128621  LyyrSgdpmDbJODdrH_GP5Q      5    y       2  0kXhQEQHGVYVA-oxK8RrFQ  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_reviews.iloc[[83656, 83000+45621]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for breaking my code, 'CVhY1u5ZN4MidNWnRC-V6g' and '0kXhQEQHGVYVA-oxK8RrFQ'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_reviews_clean = chinese_reviews.drop(chinese_reviews.index[[83656, 83000+45621]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_reviews_clean.to_csv('chinese_review_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
