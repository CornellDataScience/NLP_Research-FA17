{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from text_embedder import TextEmbedder\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained data\n",
    "business = pd.read_csv('../data/chinese_business_clean.csv')\n",
    "reviews = pd.read_csv('../data/chinese_reviews_clean_offsets.csv')\n",
    "reviews['date_tuple'] = [eval(i) for i in reviews['date_tuple']]\n",
    "\n",
    "# load gensim model\n",
    "lda =  models.LdaModel.load('../data/gensim/lda.model')\n",
    "dictionary = corpora.Dictionary.load('../data/gensim/chinsese_dict.dict')\n",
    "\n",
    "# load idf matrices\n",
    "with open('../data/u_idf.pickle', 'rb') as f:\n",
    "    uidf_data = pickle.load(f)\n",
    "with open('../data/b_idf.pickle', 'rb') as f:\n",
    "    bidf_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TextEmbedder(model = lda, dictionary = dictionary, user_idf = uidf_data, business_idf = bidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input_data, size_in, size_out, name):\n",
    "    '''\n",
    "    Implement tensor\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # weight as random normal variables\n",
    "        w = tf.Variable(tf.random_normal([size_in, size_out]), name = 'W')\n",
    "        # bias as random normal variables\n",
    "        b = tf.Variable(tf.random_normal([size_out]), name = 'B')\n",
    "        activation = tf.atan(tf.matmul(input_data, w) + b)\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_layer(input_data, size_in, size_out, name):\n",
    "    '''\n",
    "    output tensor\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # weight as random normal variables\n",
    "        w = tf.Variable(tf.random_normal([size_in, size_out]), name = 'W')\n",
    "        # bias as random normal variables\n",
    "        b = tf.Variable(tf.random_normal([size_out]), name = 'B')\n",
    "\n",
    "        activation = tf.atan(tf.matmul(input_data, w) + b)\n",
    "\n",
    "        return activation, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x,input_size, hidden, out_size, gate_dimention = 0.0, drop_out = 1.0):\n",
    "    '''\n",
    "    implement a filter gate before input\n",
    "    '''\n",
    "    # implement random filter here\n",
    "    \n",
    "    prev = input_size\n",
    "    if gate_dimention != 0.0:\n",
    "        gate = np.zeros(input_size)\n",
    "        i = random.sample(set(np.arange(input_size)), gate_dimention)\n",
    "        gate[[i]] = 1.0\n",
    "        activation = tf.multiply(x, gate)\n",
    "    else:\n",
    "        activation = x\n",
    "        \n",
    "    # add dropout layer with specified probability\n",
    "    if drop_out != 1.0:\n",
    "        activation = tf.nn.dropout(x, drop_out)\n",
    "\n",
    "    # build a series of hidden layers\n",
    "    for name,i in enumerate(hidden):\n",
    "        activation = layer(activation, prev, i, 'hiddenlayer-'+str(name))\n",
    "        prev = i\n",
    "\n",
    "    # build an output layer\n",
    "    embedding_in = activation\n",
    "    if out_size == 1:\n",
    "        out, weights = output_layer_sm(activation, hidden[-1], out_size, 'output')\n",
    "    else:\n",
    "        out, weights = output_layer(activation, hidden[-1], out_size, 'output')\n",
    "\n",
    "    return out, embedding_in, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try basic methods with January of 2013 - 2017\n",
    "def modified_split(df, year_month, embedder, t_size = 0.2, enum = 0, binary = False):\n",
    "    # select regions\n",
    "    data = df[df['date_tuple'] == year_month]\n",
    "    if binary:\n",
    "        data = data[data['offset'] != 0]\n",
    "    # create labels\n",
    "    label = data['offset'].values\n",
    "    if enum == 0: \n",
    "        embed = np.array([embedder.augmented_embed_text(t) for t in data['text'].values])\n",
    "    elif enum == 1: \n",
    "        embed = np.array([embedder.user_tfidf_embed(t, u) for t, u in zip(data['test'].values, data['user_id'].values)])\n",
    "    elif enum == 2: \n",
    "        embed = np.array([embedder.user_tf_business_idf(t, b) for t in zip(data['test'].values, data['business_id'].values)])\n",
    "    elif enum == 3: \n",
    "        embed = np.array([embedder.user_tfidf_business_idf(t, u, b) for t, u, b in zip(data['test'].values, data['user_id'].values, data['business_id'].values)])\n",
    "    elif enum == 4: \n",
    "        embed = np.array([embedder.embed(t) for t in zip(data['test'].values)])\n",
    "    elif enum == 5:\n",
    "        embed = np.array([embedder.embed_sent(t) for t in zip(data['test'].values)])\n",
    "    else:\n",
    "        print ('enum {} is not supported'.format(enum))\n",
    "        return None\n",
    "    return embed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label data, try to predict simple labels -- positive(1), negative(-1) or average(0)\n",
    "def labels(data):\n",
    "    out = []\n",
    "    for offsets in data:\n",
    "        if offsets < 0.0:\n",
    "            out.append(np.array([1.,0.,0.]))\n",
    "        elif offsets == 0.0:\n",
    "            out.append(np.array([0.,1.,0.]))\n",
    "        else:\n",
    "            out.append(np.array([0.,0.,1.0]))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labels_binary(data):\n",
    "    out = []\n",
    "    for offsets in data:\n",
    "        if offsets < 0.0:\n",
    "            out.append(np.array([1.,0.]))\n",
    "        else:\n",
    "            out.append(np.array([0.,1.0]))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_data, y_data, x_test, y_test, training_epoch, beta = 0.0, gate_size = 0.0, drop_out = 1.0, learning_rate = 0.01, hidden_layer = [100, 80], out_layer = 5):\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape = [None, 128], name = 'input_topic') # number of topics\n",
    "    y = tf.placeholder(tf.float32, shape = [None, out_layer], name = 'softmax') # 5 stars\n",
    "\n",
    "    embedded_size = hidden_layer[-1]\n",
    "    out, embedding_in, weights = build_model(x, 128, hidden_layer, out_layer, gate_size, drop_out) # shape of (?, 5)\n",
    "    \n",
    "    \n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        if out_layer == 1:\n",
    "            cross_entropy = tf.multiply(tf.reduce_mean(tf.pow(out - y, 2)), 10)\n",
    "        else:\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= out, labels = y))\n",
    "            regularizer = tf.nn.l2_loss(weights)\n",
    "            cross_entropy = tf.reduce_mean(cross_entropy + beta * regularizer)\n",
    "    # optimization\n",
    "    with tf.name_scope(\"train\"):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "    # reports\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        if out_layer == 1:\n",
    "            correct = tf.reduce_mean(tf.cast(cross_entropy , dtype = tf.float32))\n",
    "        else:\n",
    "            correct = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(y, 1)), dtype = tf.float32))\n",
    "    \n",
    "    print ('training starts ...')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(training_epoch):\n",
    "            idx = random.sample(set(np.arange(len(x_data))), 10)\n",
    "            # create embedding with embedder\n",
    "            x_in = x_data[idx]\n",
    "            y_out = y_data[idx]\n",
    "            \n",
    "            if epoch % 50000 == 0:\n",
    "                [accuracy] = sess.run([correct], feed_dict = {x:x_data[:10], y:y_data[:10]})\n",
    "                print ('%.4f' % accuracy)\n",
    "            sess.run(opt, feed_dict = {x:x_in, y:y_out})\n",
    "\n",
    "        pred = tf.nn.softmax(out)  # Apply softmax to logits\n",
    "        # Calculate accuracy\n",
    "        print(\"Accuracy:\", sess.run(correct, feed_dict = {x: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 56.2 ms, total: 19.7 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%time x0, y0 = modified_split(reviews, (2013, 1), embedder, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = labels(y0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x0, y0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.1000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "0.7000\n",
      "Accuracy: 0.433692\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 500000, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 58.7 ms, total: 19.7 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%time x1, y1 = modified_split(reviews, (2013, 1), embedder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1 = labels(y1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.3000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "Accuracy: 0.480287\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 69.2 ms, total: 19.7 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%time x2, y2 = modified_split(reviews, (2013, 1), embedder, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = labels(y2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x2, y2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.3000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "Accuracy: 0.451613\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 s, sys: 29.1 ms, total: 19 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%time x3, y3 = modified_split(reviews, (2013, 1), embedder, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y3 = labels(y3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x3, y3, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.5000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "0.2000\n",
      "Accuracy: 0.448029\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 49.7 ms, total: 12 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%time x0, y0 = modified_split(reviews, (2013, 1), embedder, 0, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y0 = labels_binary(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x0, y0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.6000\n",
      "0.6000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "Accuracy: 0.470199\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 28.9 ms, total: 11.5 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%time x1, y1 = modified_split(reviews, (2013, 1), embedder, 1, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1 = labels_binary(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.7000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "Accuracy: 0.496689\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 17.3 ms, total: 11.3 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%time x2, y2 = modified_split(reviews, (2013, 1), embedder, 2, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = labels_binary(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x2, y2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "0.4000\n",
      "Accuracy: 0.536424\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 19.1 ms, total: 11.3 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%time x3, y3 = modified_split(reviews, (2013, 1), embedder, 3, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y3 = labels_binary(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x3, y3, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts ...\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "0.5000\n",
      "Accuracy: 0.503311\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, 200000, out_layer = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
