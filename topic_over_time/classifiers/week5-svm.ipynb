{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding method round 2\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class LDAembedder(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        if kwargs:\n",
    "\n",
    "            self.vectorizer = kwargs['counter']\n",
    "\n",
    "            self.feature_names = self.vectorizer.get_feature_names()\n",
    "\n",
    "            self.lda_model = model\n",
    "\n",
    "        else:\n",
    "\n",
    "            if kwargs['max_features']:\n",
    "\n",
    "                max_features = kwargs['max_features']\n",
    "\n",
    "            else:\n",
    "\n",
    "                max_features = 100000\n",
    "\n",
    "            self.vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features = max_features)\n",
    "\n",
    "            self.feature_names = None\n",
    "\n",
    "            self.lda_model = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def display_topics(self, n_top_words, topic_n = None):\n",
    "\n",
    "        '''\n",
    "\n",
    "        Display the top n words for each topic in the model.\n",
    "\n",
    "\n",
    "\n",
    "        Input:\n",
    "\n",
    "            n_top_words(int) : the number of words to display for each topic\n",
    "\n",
    "            (Optional)\n",
    "\n",
    "            topic_n(int) : if specified, only diplay the topic_nth topic\n",
    "\n",
    "        '''\n",
    "\n",
    "        model = self.lda_model\n",
    "\n",
    "        feature_names = self.feature_names\n",
    "\n",
    "\n",
    "\n",
    "        if model and feature_names:\n",
    "\n",
    "            if topic_n:\n",
    "\n",
    "                topic = model.components_[topic_n]\n",
    "\n",
    "                print(\"Topic %d:\" % topic_n)\n",
    "\n",
    "                print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "\n",
    "            else:\n",
    "\n",
    "                for topic_index, topic in enumerate(model.components_):\n",
    "\n",
    "                    print(\"Topic %d:\" % topic_index)\n",
    "\n",
    "                    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise Exception('You have to train before display')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, texts, n = 100):\n",
    "\n",
    "        '''\n",
    "\n",
    "        Train the LDA with n number of topics\n",
    "\n",
    "\n",
    "\n",
    "        Input:\n",
    "\n",
    "            texts(list) : list of documents. Each item in the list is a string type\n",
    "\n",
    "            n(int) : number of topics, if not specified n = 100\n",
    "\n",
    "        '''\n",
    "\n",
    "        count = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "        self.feature_names = self.vectorizer.get_feature_names()\n",
    "\n",
    "        model = LatentDirichletAllocation(n_topics=n).fit(count)\n",
    "\n",
    "        self.lda_model = model\n",
    "\n",
    "\n",
    "\n",
    "    def embed(self, text, method = 'additive'):\n",
    "\n",
    "        '''\n",
    "\n",
    "        embed the review text into k-dimensional topic vector\n",
    "\n",
    "\n",
    "\n",
    "        Input:\n",
    "\n",
    "            text(string) : a document to embed\n",
    "\n",
    "            method(string) : 'additive' or 'multiplicative'\n",
    "\n",
    "\n",
    "\n",
    "        Output:\n",
    "\n",
    "            the vector of length k (k = number of topics)\n",
    "\n",
    "        '''\n",
    "\n",
    "        tokenizer = self.vectorizer.build_analyzer()\n",
    "\n",
    "        count = self.vectorizer.transform(tokenizer(text))\n",
    "\n",
    "\n",
    "\n",
    "        dirich = self.lda_model.transform(count)\n",
    "\n",
    "        if method == 'additive':\n",
    "\n",
    "            total = np.array([1]*dirich[0])\n",
    "\n",
    "            for i in dirich:\n",
    "\n",
    "                total = np.add(total, i)\n",
    "\n",
    "            return total / sum(total)\n",
    "\n",
    "        elif method == 'multiplicative':\n",
    "\n",
    "            product = np.array([1]*dirich[0])\n",
    "\n",
    "            for i in dirich:\n",
    "\n",
    "                product = np.multiply(product, i)\n",
    "\n",
    "            return product / sum(product)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return dirich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"C:\\\\Users\\\\cdchang\\\\chinese.pkl\"\n",
    "path2 = \"C:\\\\Users\\\\cdchang\\\\chinese_counter.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdchang\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.1 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\cdchang\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.18.1 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "location los new angeles judge book like restaurant cover lucky\n",
      "Topic 1:\n",
      "steak eggs rare bo pho like good meatballs hue medium\n",
      "Topic 2:\n",
      "english chinese speak food mandarin menu don like really language\n",
      "Topic 3:\n",
      "cantonese chinese tin just restaurant silver summerlin gigantic road laws\n",
      "Topic 4:\n",
      "milk coffee tea bread breakfast came drink toast like ham\n",
      "Topic 5:\n",
      "shrimp fried rice great walnut food chicken honey house good\n",
      "Topic 6:\n",
      "sour chicken sweet hot soup kung pao good hakka food\n",
      "Topic 7:\n",
      "really place food good chinese nice just try surprised mom\n",
      "Topic 8:\n",
      "congee fried rice queen dough turnip place food preserved chinese\n",
      "Topic 9:\n",
      "options variety choose fresh meat veggies lots choices meats menu\n",
      "Topic 10:\n",
      "wonderful mother family great fin shark dinner nest divine food\n",
      "Topic 11:\n",
      "ho pizza mi flavours waffle sandwich banh waffles loving sandwiches\n",
      "Topic 12:\n",
      "shrimp rice pork mai siu fried steamed ribs egg har\n",
      "Topic 13:\n",
      "chicken food ordered like sauce just meat tasted soggy dry\n",
      "Topic 14:\n",
      "shrimp pepper salt like sauce ordered flavor just beef chicken\n",
      "Topic 15:\n",
      "cash card credit moo cards accept debit gai pay shu\n",
      "Topic 16:\n",
      "kids son like eat kid daughter really dad place good\n",
      "Topic 17:\n",
      "dragon sister boat just states restaurant mushu dull way emperor\n",
      "Topic 18:\n",
      "crab general chicken tso good puffs rangoon rice tao fried\n",
      "Topic 19:\n",
      "tea bubble iced green hot cup teas water free jasmine\n",
      "Topic 20:\n",
      "dont tips discount pong pay ping pang gold casino don\n",
      "Topic 21:\n",
      "service food table waitress asked came waiter just restaurant didn\n",
      "Topic 22:\n",
      "time 2nd store 1st grocery 3rd visit food restaurant truck\n",
      "Topic 23:\n",
      "food place bad worst money terrible eat horrible just don\n",
      "Topic 24:\n",
      "minutes order food time took 10 ordered wait 15 waited\n",
      "Topic 25:\n",
      "view fake christmas bellagio id fountain jasmine eve wearing wear\n",
      "Topic 26:\n",
      "mochi durian tasteful charm meets massage locally beverages heavenly tony\n",
      "Topic 27:\n",
      "hunan dishes college students food place good cheap student campus\n",
      "Topic 28:\n",
      "game play place board just games like hang really little\n",
      "Topic 29:\n",
      "order food customer service don just said time like want\n",
      "Topic 30:\n",
      "soup dumpling dumplings won ton shanghai pork good just order\n",
      "Topic 31:\n",
      "like don just water place know food oh people want\n",
      "Topic 32:\n",
      "noodle beef soup taiwanese good place noodles really like tofu\n",
      "Topic 33:\n",
      "ve times food good best place chinese time tried eaten\n",
      "Topic 34:\n",
      "vegetarian korean veggie vegan delicious dishes menu great loved food\n",
      "Topic 35:\n",
      "asked menu dish said ordered wanted coupon told did waitress\n",
      "Topic 36:\n",
      "buffet sushi buffets legs crab good food like selection fresh\n",
      "Topic 37:\n",
      "place food gem hidden phoenix chinese great area best drive\n",
      "Topic 38:\n",
      "95 menu restaurant decided served dining ordered place meal fellow\n",
      "Topic 39:\n",
      "gf inn belt penny conveyor joy wheat cutlery prawn owns\n",
      "Topic 40:\n",
      "chinese dishes restaurant food authentic menu good cuisine place traditional\n",
      "Topic 41:\n",
      "soup wonton table restaurant meal hot menu chai kee just\n",
      "Topic 42:\n",
      "sea bass choy bok scallops decor baby buddha beautiful mouth\n",
      "Topic 43:\n",
      "bakery like place decorations store pastries goods chinese parking don\n",
      "Topic 44:\n",
      "food service slow good night came seated busy wait quickly\n",
      "Topic 45:\n",
      "love place food best great amazing favorite awesome friendly family\n",
      "Topic 46:\n",
      "timely door place manner food prime time fiance grade went\n",
      "Topic 47:\n",
      "la pour carte resto ma restos adore entre bon encore\n",
      "Topic 48:\n",
      "lettuce ago wraps pf chang years changs good weeks chain\n",
      "Topic 49:\n",
      "chinese food china restaurant like american new real york good\n",
      "Topic 50:\n",
      "mein chow lo wei pei good mien chicken order food\n",
      "Topic 51:\n",
      "rice fried chicken ordered white pork like deep got food\n",
      "Topic 52:\n",
      "fries cheese burger french toast bacon potato hawaiian short rib\n",
      "Topic 53:\n",
      "chinese food asian restaurants restaurant authentic americanized don place like\n",
      "Topic 54:\n",
      "bowl 99 50 meal yc soup meat sauce grill drink\n",
      "Topic 55:\n",
      "dimsum squid leaves 88 desired pea snow 80 ocean tentacles\n",
      "Topic 56:\n",
      "chef piece dish plate experience dishes meal ordered pieces wing\n",
      "Topic 57:\n",
      "like food tasted taste old looked place smell just ordered\n",
      "Topic 58:\n",
      "delivery order food delivered ordered ordering online time deliver phone\n",
      "Topic 59:\n",
      "good food selection pretty service decent place im overall price\n",
      "Topic 60:\n",
      "tofu sauce bean garlic eggplant black chinese green onions fried\n",
      "Topic 61:\n",
      "tables dirty dining room table chairs plates sticky food place\n",
      "Topic 62:\n",
      "healthy calories liquor ginger throat msg root garlic alternative like\n",
      "Topic 63:\n",
      "style hong kong food place good time like restaurant cantonese\n",
      "Topic 64:\n",
      "hainan ca rice boss brings casserole shake organic meal memories\n",
      "Topic 65:\n",
      "portion parking quite small size food court price good lot\n",
      "Topic 66:\n",
      "spicy pork szechuan chili dish lamb oil pancake sichuan dishes\n",
      "Topic 67:\n",
      "lobster tip seafood crab dinner came tax king set people\n",
      "Topic 68:\n",
      "oysters 100 oyster seafood pearl dozen mussels clam dang wongs\n",
      "Topic 69:\n",
      "long wait busy early time come good weekends line table\n",
      "Topic 70:\n",
      "fusion like st asian place yonge casual interesting bit just\n",
      "Topic 71:\n",
      "dim sum carts cart place good ladies feet come came\n",
      "Topic 72:\n",
      "lee feels cons pros food rushed like wedding fancy good\n",
      "Topic 73:\n",
      "food place service good lamb recommend worth skewers great excellent\n",
      "Topic 74:\n",
      "et le est les pas en que des je une\n",
      "Topic 75:\n",
      "pot hot stickers broth hotpot spicy soup beef base meat\n",
      "Topic 76:\n",
      "mexican tacos taco chips concept china fly fusion poblano food\n",
      "Topic 77:\n",
      "tea milk mango like drink drinks sweet dessert boba sugar\n",
      "Topic 78:\n",
      "beef great mongolian soup food highly place service recommend delicious\n",
      "Topic 79:\n",
      "hk cafe style baked rice soup spaghetti good pasta dishes\n",
      "Topic 80:\n",
      "chicken wings beef broccoli ordered delicious husband food crispy definitely\n",
      "Topic 81:\n",
      "fish balls ball filet good fillet tank salted soup like\n",
      "Topic 82:\n",
      "club floor people music dance hakkasan night line girls like\n",
      "Topic 83:\n",
      "food place wall buffet hole chinese eat cheap loves good\n",
      "Topic 84:\n",
      "stir sauces fry ingredients bowls veggies add make cook good\n",
      "Topic 85:\n",
      "menu just dishes restaurant good food chopstix memory socks items\n",
      "Topic 86:\n",
      "egg soup wonton rolls roll drop good chicken chinese wontons\n",
      "Topic 87:\n",
      "sauce soy flavor sweet salad chicken peanut like chili just\n",
      "Topic 88:\n",
      "chicken beans red jade jerk rice food black fried place\n",
      "Topic 89:\n",
      "strip mall boba place breakfast good located snoh ice grub\n",
      "Topic 90:\n",
      "hot pot don place like eat clay people pots know\n",
      "Topic 91:\n",
      "server table came manager did brought party meal asked told\n",
      "Topic 92:\n",
      "wasn just didn like really good got ordered pretty came\n",
      "Topic 93:\n",
      "tai fung legend ding grace ha saving nyc montreal imperial\n",
      "Topic 94:\n",
      "night late open place food hours till time groupon closed\n",
      "Topic 95:\n",
      "und die das der ist nicht ich war es sehr\n",
      "Topic 96:\n",
      "location panda express food locations employees better fresh like drive\n",
      "Topic 97:\n",
      "restaurant place quality beef like interior new good price menu\n",
      "Topic 98:\n",
      "good place lol cousin damn got eat just oh holy\n",
      "Topic 99:\n",
      "pork belly bao buns bun fat steamed good baos just\n",
      "Topic 100:\n",
      "noodles hand dan pulled noodle watch fresh spicy singapore cut\n",
      "Topic 101:\n",
      "food fast place priced delivery order chinese reasonably good fresh\n",
      "Topic 102:\n",
      "dish sauce fried beef dishes cooked good meat chicken sweet\n",
      "Topic 103:\n",
      "dumplings steamed fried pan pork dumpling good delicious onion place\n",
      "Topic 104:\n",
      "chicken orange sesame good chinese like rice sauce food fried\n",
      "Topic 105:\n",
      "ramen broth japanese bar buns pork momofuku great noodles kimchi\n",
      "Topic 106:\n",
      "people restaurant waiting just table wait line tables counter seat\n",
      "Topic 107:\n",
      "food good service great place prices chinese friendly fast quick\n",
      "Topic 108:\n",
      "buddy 21 addicted ah gardens proportions dissapointed 3x fist lies\n",
      "Topic 109:\n",
      "noodle soup noodles beef broth bowl brisket good base wonton\n",
      "Topic 110:\n",
      "00 young fortune foo egg cookies cookie wine gravy charge\n",
      "Topic 111:\n",
      "dim sum place good service dishes places restaurant better food\n",
      "Topic 112:\n",
      "forward golden menu expectations looking look restaurant got exceeded items\n",
      "Topic 113:\n",
      "duck peking roast roasted skin crispy good pork chinese meat\n",
      "Topic 114:\n",
      "thai curry pad spicy chicken good food tom ordered spice\n",
      "Topic 115:\n",
      "wong truffle chef delicious watermelon delighted day asian big brilliant\n",
      "Topic 116:\n",
      "bbq pork chop com rice http www yelp select meat\n",
      "Topic 117:\n",
      "great food service staff friendly experience amazing attentive time place\n",
      "Topic 118:\n",
      "rolls spring good calamari open pretty food place got really\n",
      "Topic 119:\n",
      "sushi rolls good bar great place fresh happy menu hour\n",
      "Topic 120:\n",
      "coke diet switch ox skewer fare tail varied earth venetian\n",
      "Topic 121:\n",
      "roll tuna salmon sushi tempura spicy good rolls miso ordered\n",
      "Topic 122:\n",
      "reviews yelp review place read negative food reading new previous\n",
      "Topic 123:\n",
      "lunch special specials good food price dinner place really just\n",
      "Topic 124:\n",
      "pho vegas las vietnamese place good best strip 24 long\n",
      "Topic 125:\n",
      "season pinch lomein air insult moderate wowed ticket chains dundas\n",
      "Topic 126:\n",
      "food health good alot safe blah forever eat service department\n",
      "Topic 127:\n",
      "dessert ice cream cake coconut chocolate curry mango roti banana\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00390625\n"
     ]
    }
   ],
   "source": [
    "#testing the new embedder\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "# load the pre-trained model\n",
    "with open(path2, \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "with open(path1, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "lda = LDAembedder(model = model, counter = vectorizer)\n",
    "\n",
    "# print top 10 words from each topic\n",
    "lda.display_topics(10)\n",
    "\n",
    "# embed text\n",
    "sample = 'This place is horrible, we were so excited to try it since I got a gift card for my birthday. We went in an ordered are whole meal and they did not except are gift card, because their system was down. Unacceptable, this would have been so helpful if we would have known this prior!!'\n",
    "\n",
    "add = lda.embed(sample, 'additive')\n",
    "prod = lda.embed(sample, 'multiplicative')\n",
    "\n",
    "print(add[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json file setup in notebook\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_df(datapass):\n",
    "    '''\n",
    "    Load the json file and parse the file to pandas dataframe format\n",
    "    \n",
    "    Input:\n",
    "        datapass(str) : directory to the json file\n",
    "    Output:\n",
    "        df(dataframe) : pandas dataframe object\n",
    "    '''\n",
    "    \n",
    "    data = [] \n",
    "    with open(datapass, 'r', encoding='utf-8') as data_file: \n",
    "        for f in data_file:\n",
    "            data.append(json.loads(f))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "#business and review json files --> dataframes\n",
    "\n",
    "DATAPASS1 = 'C:\\\\Users\\\\cdchang\\\\Downloads\\\\yelp_dataset\\\\dataset\\\\business.json'\n",
    "DATAPASS2 = 'C:\\\\Users\\\\cdchang\\\\Downloads\\\\yelp_dataset\\\\dataset\\\\review.json'\n",
    "\n",
    "#business = json_df(DATAPASS1)\n",
    "reviews = json_df(DATAPASS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old methods from past weeks\n",
    "\n",
    "def business_id_retrieval(cat, business):\n",
    "    '''\n",
    "    Input:\n",
    "        cat(str) : category\n",
    "        business(dataframe) : the business data\n",
    "    Output:\n",
    "        id_list(set) : business ids of a particular category\n",
    "    '''\n",
    "    id_list = set()\n",
    "    id_list = []\n",
    "    idx = 0\n",
    "    for row in business.values:\n",
    "        categories = row[3]\n",
    "        if cat in categories:\n",
    "            id_list.append(row[2])\n",
    "            \n",
    "    return id_list\n",
    "\n",
    "def examine_reviews2(cat, business, reviews): #across entire category\n",
    "    '''\n",
    "    concatenate a category's review text with the corresponding dates for time series analysis\n",
    "    Input: specific category, review json file\n",
    "    Output: concatenated review text & date columns for a particular category (not business specific) \n",
    "    '''\n",
    "    id_list = business_id_retrieval(cat, business)\n",
    "    return reviews.loc[reviews['business_id'].isin(id_list)]\n",
    "\n",
    "def examine_reviews(b_id, reviews):\n",
    "    '''\n",
    "    concatenate a particular business's review text with the corresponding dates for time series analysis\n",
    "    \n",
    "    Input: specific business id, review json file\n",
    "    Output: concatenated review text & date columns for a particular business id\n",
    "    '''\n",
    "    subset = reviews.loc[reviews['business_id'] == b_id]\n",
    "    date = subset.loc[:, 'date']\n",
    "    text = subset.loc[:, 'text']\n",
    "    return pd.concat([date, text], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16      2017-06-03\n",
       "17      2015-03-26\n",
       "18      2012-12-30\n",
       "19      2009-01-12\n",
       "20      2015-07-11\n",
       "21      2015-05-27\n",
       "22      2015-02-28\n",
       "23      2010-04-05\n",
       "24      2015-05-22\n",
       "25      2011-06-15\n",
       "26      2017-03-12\n",
       "27      2016-12-19\n",
       "28      2011-08-10\n",
       "29      2013-06-17\n",
       "30      2015-11-03\n",
       "31      2017-07-08\n",
       "32      2015-09-22\n",
       "33      2012-10-08\n",
       "34      2015-12-28\n",
       "35      2009-11-18\n",
       "36      2012-07-29\n",
       "37      2016-06-19\n",
       "38      2012-08-23\n",
       "39      2009-06-25\n",
       "40      2008-10-06\n",
       "41      2015-03-17\n",
       "42      2014-04-22\n",
       "43      2016-05-07\n",
       "44      2014-05-03\n",
       "45      2014-05-29\n",
       "           ...    \n",
       "68      2013-05-23\n",
       "69      2017-04-07\n",
       "70      2016-03-17\n",
       "71      2015-08-08\n",
       "72      2008-11-27\n",
       "73      2017-06-10\n",
       "74      2013-04-26\n",
       "75      2014-12-27\n",
       "76      2017-04-23\n",
       "77      2016-03-02\n",
       "78      2017-06-12\n",
       "79      2009-01-19\n",
       "80      2010-12-29\n",
       "81      2016-08-28\n",
       "82      2015-11-07\n",
       "83      2010-11-19\n",
       "84      2015-03-15\n",
       "85      2011-09-15\n",
       "86      2014-10-27\n",
       "87      2017-03-06\n",
       "88      2017-01-24\n",
       "9523    2015-03-07\n",
       "9524    2017-06-02\n",
       "9525    2015-01-23\n",
       "9526    2013-08-23\n",
       "9527    2011-10-22\n",
       "9528    2016-03-20\n",
       "9529    2012-10-09\n",
       "9530    2015-03-30\n",
       "9531    2017-05-14\n",
       "Name: date, Length: 82, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#end of set-up cells\n",
    "#pre-processing / grouping of appropriate data\n",
    "#TODO: we need \n",
    "compilation = examine_reviews2(\"Chinese\", business, reviews)\n",
    "idlist = business_id_retrieval(\"Chinese\", business)\n",
    "singular_subset = examine_reviews(\"jQsNFOzDpxPmOurSWCg1vQ\", reviews)\n",
    "\n",
    "\n",
    "def df_allcomp(b_id):\n",
    "    subset = reviews.loc[reviews['business_id']==b_id]\n",
    "    date = subset.loc[:,'date']\n",
    "    text = subset.loc[:,'text']\n",
    "    stars = subset.loc[:,'stars']\n",
    "\n",
    "    df = pd.concat([date,text,stars],axis=1, join='inner')\n",
    "    return df\n",
    "\n",
    "#still needs to be ordered\n",
    "df1 = df_allcomp(\"jQsNFOzDpxPmOurSWCg1vQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [92, 82]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e4ff14e13d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#x should be a cerain number of topics, y should be star rating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 173\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [92, 82]"
     ]
    }
   ],
   "source": [
    "#topic filtering ....\n",
    "\n",
    "import numpy as np\n",
    "#svm\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma = 0.001, C=100)\n",
    "x = np.array(range(0,92))\n",
    "newx = x.reshape(-1,1)\n",
    "y = df1.stars[:-10]\n",
    "#x should be a cerain number of topics, y should be star rating\n",
    "clf.fit(newx,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
